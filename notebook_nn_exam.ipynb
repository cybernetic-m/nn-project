{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dIRawMyjYAz"
      },
      "source": [
        "⚠️Clone the repository to use all the functions needed by the main code⚠️\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvEJSRQzjVVO",
        "outputId": "621797ba-2595-461d-e6ec-8c898391b5dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are running locally!\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    mode = 'colab'\n",
        "except:\n",
        "    mode = 'local'\n",
        "\n",
        "if mode == 'colab':\n",
        "    !git clone \"https://github.com/cybernetic-m/nn-project.git\"\n",
        "    !pip install hydra-core --upgrade convkan\n",
        "else:\n",
        "    print(\"You are running locally!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZNKzy8qbbu2"
      },
      "source": [
        "# IMPORT AND INITIALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dKakfnMRbKum"
      },
      "outputs": [],
      "source": [
        "# Import for config.yaml file\n",
        "from hydra import initialize, compose\n",
        "from hydra.core.global_hydra import GlobalHydra\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import Audio\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "if mode == 'colab':\n",
        "    sys.path.append('/content/nn-project/dataloader')\n",
        "    sys.path.append('/content/nn-project/model')\n",
        "    sys.path.append('/content/nn-project/module')\n",
        "    sys.path.append('/content/nn-project/training')\n",
        "    sys.path.append('/content/nn-project/testing')\n",
        "    from preprocessing import Preprocessing\n",
        "    import utils\n",
        "    import dataset\n",
        "    from ctim import CTIM\n",
        "    from train import *\n",
        "    from test import *\n",
        "\n",
        "\n",
        "if mode == 'local':\n",
        "    import dataloader.utils as utils\n",
        "    import dataloader.dataset as dataset\n",
        "    from dataloader.preprocessing import Preprocessing\n",
        "    from model.ctim import CTIM\n",
        "    from training.train import train as train\n",
        "    from testing.test import test as test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzpebiXBPJE-"
      },
      "source": [
        "Reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "X3f4K2m8PIrc"
      },
      "outputs": [],
      "source": [
        "# Set the seed\n",
        "seed = 46\n",
        "\n",
        "# Set seed for torch, numpy and random libraries\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# Set the devide mode on GPU (if available CUDA for Nvidia and  MPS for Apple Silicon) or CPU\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mV42Zq-UKCz"
      },
      "source": [
        "## Download and Split Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV_poPYiccs2",
        "outputId": "e50c5a1d-7044-4167-99c3-48ec2f05c711"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset already downloaded\n",
            "Dataset already unzipped\n"
          ]
        }
      ],
      "source": [
        "link_dataset = \"https://drive.google.com/file/d/1nzKBta2M3khw7Ql_S7atYg-H-bWDiOxr/view?usp=drive_link\"\n",
        "gdrive_link = \"https://drive.google.com/uc?export=download&id=\"\n",
        "\n",
        "if mode == 'colab':\n",
        "    destination_dir = \"/content/emovo.zip\"\n",
        "    extract_dir = '/content/dataset'\n",
        "    emovo_dir = '/content/dataset/EMOVO'\n",
        "    emovo_split_dir = '/content/dataset/EMOVO_split'\n",
        "elif mode == 'local':\n",
        "    destination_dir = \"./emovo.zip\"\n",
        "    extract_dir = \"./dataset\"\n",
        "    emovo_dir = \"./dataset/EMOVO\"\n",
        "    emovo_split_dir = './dataset/EMOVO_split'\n",
        "\n",
        "utils.download_dataset(link_dataset, destination_dir, gdrive_link, extract_dir)\n",
        "\n",
        "utils.dataset_split(emovo_dir, extract_dir, 0.7, 0.2, 0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "utils.dataset_split_mfcc('./EMOVO.npy', extract_dir, 0.7, 0.2, 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrZ8n_tp2nfh"
      },
      "source": [
        "# Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBRzIeeY2nfh",
        "outputId": "b7a2bfe4-c0e0-40ab-94c7-f9a3afffa6d9"
      },
      "outputs": [],
      "source": [
        "preprocessing = Preprocessing(device=device)\n",
        "\n",
        "utils.augment_data(emovo_split_dir, extract_dir, preprocessing, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVRKvN26UUq9"
      },
      "source": [
        "## Dataset Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRn6VibgUXm9",
        "outputId": "491fc476-55ab-423a-f653-619cae3944dc"
      },
      "outputs": [],
      "source": [
        "train_path = os.path.join(extract_dir, 'EMOVO_split', 'train')\n",
        "test_path = os.path.join(extract_dir, 'EMOVO_split', 'test')\n",
        "val_path = os.path.join(extract_dir, 'EMOVO_split', 'val')\n",
        "\n",
        "train_dataset = dataset.EMOVO_Dataset(train_path, False, device=device)\n",
        "test_dataset = dataset.EMOVO_Dataset(test_path, False, device=device)\n",
        "val_dataset = dataset.EMOVO_Dataset(val_path, False, device=device)\n",
        "\n",
        "train_aug_path = os.path.join(extract_dir, 'EMOVO_aug', 'train')\n",
        "test_aug_path = os.path.join(extract_dir, 'EMOVO_aug', 'test')\n",
        "val_aug_path = os.path.join(extract_dir, 'EMOVO_aug', 'val')\n",
        "\n",
        "train_aug_dataset = dataset.EMOVO_Dataset(train_aug_path, device=device)\n",
        "test_aug_dataset = dataset.EMOVO_Dataset(test_aug_path, device=device)\n",
        "val_aug_dataset = dataset.EMOVO_Dataset(val_aug_path, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ9URMKkfQmU"
      },
      "source": [
        "Example of audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "mcblPPkeZw35",
        "outputId": "f79994e1-f311-4cb7-f112-de385323b3ae"
      },
      "outputs": [],
      "source": [
        "classes = ['dis', 'gio', 'neu', 'pau', 'rab', 'sor', 'tri']\n",
        "random_num = random.randint(1, train_dataset.__len__())\n",
        "data, label = train_dataset[random_num]\n",
        "\n",
        "print(\"Tensor of shape:\",data[0].shape, \"Sample Rate:\", data[1])\n",
        "print(\"Class:\", classes[label], \"\\n\")\n",
        "\n",
        "# Waveform\n",
        "# Compute the average of both channels to get a mono waveform\n",
        "mono_waveform = data[0].mean(dim=0)\n",
        "num_samples = mono_waveform.shape[0]\n",
        "sample_rate = data[1]\n",
        "time_axis = np.linspace(0, num_samples / sample_rate, num_samples)\n",
        "\n",
        "# Play the audio\n",
        "display(Audio(mono_waveform.cpu().numpy(), rate=sample_rate))\n",
        "\n",
        "# Plot the averaged (mono) waveform\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(time_axis, mono_waveform.cpu().numpy())\n",
        "plt.title('Waveform (Averaged Mono)')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSzFkhBu2nfi"
      },
      "source": [
        "Example of augmented audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5XiV4x82nfi"
      },
      "outputs": [],
      "source": [
        "# TO DO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnwkpxVjIDni"
      },
      "source": [
        "Data distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "q3Hs-83fIHPT",
        "outputId": "f64ac95c-b03e-4b4b-9156-c7e93f23e3b8"
      },
      "outputs": [],
      "source": [
        "train_counts = train_dataset.get_info()\n",
        "test_counts = test_dataset.get_info()\n",
        "val_counts = val_dataset.get_info()\n",
        "classes = ['dis', 'gio', 'neu', 'pau', 'rab', 'sor', 'tri']\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1,3, figsize=(16, 4))\n",
        "\n",
        "ax[0].bar(classes, train_counts, color='limegreen')\n",
        "ax[0].set_title(\"Data distribution in Training Set\")\n",
        "ax[0].set_xlabel(\"Classes\")\n",
        "ax[0].set_xticks(ticks=range(7), labels=classes)\n",
        "ax[0].set_ylabel(\"Number of samples\")\n",
        "\n",
        "ax[1].bar(classes, test_counts, color='seagreen')\n",
        "ax[1].set_title(\"Data distribution in Test Set\")\n",
        "ax[1].set_xlabel(\"Classes\")\n",
        "ax[1].set_xticks(ticks=range(7), labels=classes)\n",
        "ax[1].set_ylabel(\"Number of samples\")\n",
        "\n",
        "ax[2].bar(classes, val_counts, color='olive')\n",
        "ax[2].set_title(\"Data distribution in Validation Set\")\n",
        "ax[2].set_xlabel(\"Classes\")\n",
        "ax[2].set_xticks(ticks=range(7), labels=classes)\n",
        "ax[2].set_ylabel(\"Number of samples\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzyjYYHp2nfi"
      },
      "source": [
        "Data augmented distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "WUqtdsOf2nfi",
        "outputId": "3d60a842-b1f0-41c5-f68d-d605dbfcbd14"
      },
      "outputs": [],
      "source": [
        "train_aug_counts = train_aug_dataset.get_info()\n",
        "test_aug_counts = test_aug_dataset.get_info()\n",
        "val_aug_counts = val_aug_dataset.get_info()\n",
        "classes = ['dis', 'gio', 'neu', 'pau', 'rab', 'sor', 'tri']\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1,3, figsize=(16, 4))\n",
        "\n",
        "ax[0].bar(classes, train_aug_counts, color='limegreen')\n",
        "ax[0].set_title(\"Data distribution in Training Set\")\n",
        "ax[0].set_xlabel(\"Classes\")\n",
        "ax[0].set_xticks(ticks=range(7), labels=classes)\n",
        "ax[0].set_ylabel(\"Number of samples\")\n",
        "\n",
        "ax[1].bar(classes, test_aug_counts, color='seagreen')\n",
        "ax[1].set_title(\"Data distribution in Test Set\")\n",
        "ax[1].set_xlabel(\"Classes\")\n",
        "ax[1].set_xticks(ticks=range(7), labels=classes)\n",
        "ax[1].set_ylabel(\"Number of samples\")\n",
        "\n",
        "ax[2].bar(classes, val_aug_counts, color='olive')\n",
        "ax[2].set_title(\"Data distribution in Validation Set\")\n",
        "ax[2].set_xlabel(\"Classes\")\n",
        "ax[2].set_xticks(ticks=range(7), labels=classes)\n",
        "ax[2].set_ylabel(\"Number of samples\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6u2yOd0iuRt"
      },
      "source": [
        "# HYDRA INITIALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LK9-Zzs-iuRt",
        "outputId": "e32700b7-b64a-4601-cd21-6b343b4a1365"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hydra now initialized!\n"
          ]
        }
      ],
      "source": [
        "# Check if Hydra is initialized\n",
        "if GlobalHydra().is_initialized():\n",
        "    # Clear the Hydra instance if it is initialized\n",
        "    GlobalHydra.instance().clear()\n",
        "    print(\"Hydra instance was initialized and has been cleared.\")\n",
        "else:\n",
        "    # Initialize\n",
        "    print(\"Hydra now initialized!\")\n",
        "\n",
        "# Initialization and Load configuration\n",
        "if mode == 'local':\n",
        "  initialize(config_path=\"./conf\", job_name=\"notebook_nn_exam\", version_base=None)\n",
        "  cfg = compose(config_name=\"config\")\n",
        "\n",
        "elif mode == 'colab':\n",
        "  initialize(config_path=\"./nn-project/conf\", job_name=\"notebook_nn_exam\", version_base=None)\n",
        "  cfg = compose(config_name=\"config\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Vn5O9hp_iuRt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------+------------+\n",
            "|                       Modules                       | Parameters |\n",
            "+-----------------------------------------------------+------------+\n",
            "|                   ctim_net.weights                  |     8      |\n",
            "|             ctim_net.conv_forward.weight            |    1521    |\n",
            "|              ctim_net.conv_forward.bias             |     39     |\n",
            "|             ctim_net.conv_reverse.weight            |    1521    |\n",
            "|              ctim_net.conv_reverse.bias             |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.0.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.0.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.0.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.0.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.0.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.0.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.0.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.0.batch_norm2.bias  |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.1.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.1.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.1.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.1.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.1.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.1.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.1.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.1.batch_norm2.bias  |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.2.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.2.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.2.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.2.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.2.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.2.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.2.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.2.batch_norm2.bias  |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.3.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.3.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.3.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.3.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.3.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.3.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.3.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.3.batch_norm2.bias  |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.4.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.4.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.4.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.4.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.4.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.4.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.4.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.4.batch_norm2.bias  |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.5.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.5.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.5.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.5.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.5.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.5.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.5.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.5.batch_norm2.bias  |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.6.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.6.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.6.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.6.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.6.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.6.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.6.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.6.batch_norm2.bias  |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.7.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.7.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.7.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.7.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.7.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.7.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.7.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.7.batch_norm2.bias  |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.0.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.0.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.0.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.0.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.0.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.0.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.0.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.0.batch_norm2.bias  |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.1.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.1.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.1.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.1.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.1.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.1.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.1.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.1.batch_norm2.bias  |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.2.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.2.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.2.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.2.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.2.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.2.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.2.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.2.batch_norm2.bias  |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.3.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.3.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.3.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.3.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.3.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.3.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.3.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.3.batch_norm2.bias  |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.4.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.4.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.4.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.4.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.4.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.4.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.4.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.4.batch_norm2.bias  |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.5.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.5.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.5.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.5.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.5.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.5.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.5.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.5.batch_norm2.bias  |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.6.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.6.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.6.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.6.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.6.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.6.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.6.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.6.batch_norm2.bias  |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.7.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.7.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.7.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.7.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.7.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.7.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.7.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.7.batch_norm2.bias  |     39     |\n",
            "|                  classifier.weight                  |    273     |\n",
            "|                   classifier.bias                   |     7      |\n",
            "+-----------------------------------------------------+------------+\n",
            "Total Trainable Params: 104496\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "104496"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the hyperparameters of the model\n",
        "DROPOUT_RATE = cfg.model.dropout_rate\n",
        "KERNEL_SIZE = cfg.model.kernel_size\n",
        "N_TAB = cfg.model.n_temporal_aware_block\n",
        "N_FILTER = cfg.model.n_filter\n",
        "NUM_FEATURES = cfg.model.num_features\n",
        "CK = cfg.model.ck\n",
        "USE_KAN = cfg.model.use_kan\n",
        "OMEGA_0 = cfg.model.omega_0\n",
        "IS_SIREN = cfg.model.is_siren\n",
        "\n",
        "model = CTIM(\n",
        "    kernel_size=KERNEL_SIZE,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    n_temporal_aware_block=N_TAB,\n",
        "    n_filter=N_FILTER,\n",
        "    in_channels=39,\n",
        "    ck=CK,\n",
        "    num_features=NUM_FEATURES,\n",
        "    num_classes = 7,\n",
        "    use_kan = USE_KAN,\n",
        "    omega_0=OMEGA_0,\n",
        "    is_siren=IS_SIREN,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "def count_parameters(model):\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad:\n",
        "            continue\n",
        "        params = parameter.numel()\n",
        "        table.add_row([name, params])\n",
        "        total_params += params\n",
        "    print(table)\n",
        "    print(f\"Total Trainable Params: {total_params}\")\n",
        "    return total_params\n",
        "    \n",
        "count_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUdCPiRZiuRu"
      },
      "source": [
        "# TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_path = os.path.join(extract_dir, 'EMOVO_split_MFCC', 'train')\n",
        "test_path = os.path.join(extract_dir, 'EMOVO_split_MFCC', 'test')\n",
        "val_path = os.path.join(extract_dir, 'EMOVO_split_MFCC', 'val')\n",
        "\n",
        "\n",
        "# \n",
        "train_dataset = dataset.EMOVO_Dataset(train_path, feature_extract=False, mfcc_np=True, device=device)\n",
        "test_dataset = dataset.EMOVO_Dataset(test_path, feature_extract=False, mfcc_np=True, device=device)\n",
        "val_dataset = dataset.EMOVO_Dataset(val_path, feature_extract=False, mfcc_np=True, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EmPUS-iBiuRu",
        "outputId": "58030685-a642-4cea-ff80-09af8c680bac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 1/30:\n",
            "TRAIN:\n",
            "accuracy: 16.20, precision: 9.19, recall: 16.21, f1-score: 10.47\n",
            "VALIDATION:\n",
            "accuracy: 15.56, precision: 6.59, recall: 15.00, f1-score: 8.40\n",
            "LOSS train 1.97249702612559 valid 2.006108601888021\n",
            "saved: /Users/cyber_m/Desktop/Neural Network/nn-project/training/results/TIM2024-10-03_16:34:52/model.pt\n",
            "EPOCH 2/30:\n",
            "TRAIN:\n",
            "accuracy: 15.82, precision: 7.04, recall: 15.79, f1-score: 9.61\n",
            "VALIDATION:\n",
            "accuracy: 17.04, precision: 12.67, recall: 16.43, f1-score: 11.72\n",
            "LOSS train 1.970460573832194 valid 2.003174344698588\n",
            "saved: /Users/cyber_m/Desktop/Neural Network/nn-project/training/results/TIM2024-10-03_16:34:54/model.pt\n",
            "EPOCH 3/30:\n",
            "TRAIN:\n",
            "accuracy: 13.75, precision: 9.91, recall: 13.74, f1-score: 9.52\n",
            "VALIDATION:\n",
            "accuracy: 14.07, precision: 9.21, recall: 13.57, f1-score: 9.66\n",
            "LOSS train 1.9865806102752686 valid 2.022661805152893\n",
            "EPOCH 4/30:\n",
            "TRAIN:\n",
            "accuracy: 14.69, precision: 14.12, recall: 14.77, f1-score: 10.27\n",
            "VALIDATION:\n",
            "accuracy: 12.59, precision: 17.85, recall: 12.50, f1-score: 10.70\n",
            "LOSS train 1.976989467938741 valid 2.0271929105122886\n",
            "EPOCH 5/30:\n",
            "TRAIN:\n",
            "accuracy: 12.43, precision: 8.58, recall: 12.56, f1-score: 8.00\n",
            "VALIDATION:\n",
            "accuracy: 17.78, precision: 15.73, recall: 17.73, f1-score: 13.13\n",
            "LOSS train 2.0001332230038114 valid 1.9290660619735718\n",
            "saved: /Users/cyber_m/Desktop/Neural Network/nn-project/training/results/TIM2024-10-03_16:34:59/model.pt\n",
            "EPOCH 6/30:\n",
            "TRAIN:\n",
            "accuracy: 13.37, precision: 7.75, recall: 13.57, f1-score: 8.15\n",
            "VALIDATION:\n",
            "accuracy: 15.56, precision: 18.68, recall: 15.28, f1-score: 11.69\n",
            "LOSS train 1.9769133064481947 valid 1.9906566540400188\n",
            "EPOCH 7/30:\n",
            "TRAIN:\n",
            "accuracy: 12.81, precision: 10.32, recall: 12.92, f1-score: 9.18\n",
            "VALIDATION:\n",
            "accuracy: 16.30, precision: 14.41, recall: 15.79, f1-score: 10.84\n",
            "LOSS train 1.9979960918426514 valid 1.9208865960439045\n",
            "saved: /Users/cyber_m/Desktop/Neural Network/nn-project/training/results/TIM2024-10-03_16:35:03/model.pt\n",
            "EPOCH 8/30:\n",
            "TRAIN:\n",
            "accuracy: 13.37, precision: 9.32, recall: 13.44, f1-score: 8.79\n",
            "VALIDATION:\n",
            "accuracy: 16.30, precision: 8.25, recall: 15.87, f1-score: 8.52\n",
            "LOSS train 1.9941262669033475 valid 1.9937549829483032\n",
            "EPOCH 9/30:\n",
            "TRAIN:\n",
            "accuracy: 15.63, precision: 16.22, recall: 15.56, f1-score: 10.97\n",
            "VALIDATION:\n",
            "accuracy: 12.59, precision: 11.18, recall: 12.54, f1-score: 10.34\n",
            "LOSS train 1.9706568055682712 valid 2.002052664756775\n",
            "EPOCH 10/30:\n",
            "TRAIN:\n",
            "accuracy: 14.88, precision: 11.11, recall: 14.55, f1-score: 8.38\n",
            "VALIDATION:\n",
            "accuracy: 15.56, precision: 7.01, recall: 15.00, f1-score: 5.13\n",
            "LOSS train 1.980660319328308 valid 2.0158280531565347\n",
            "EPOCH 11/30:\n",
            "TRAIN:\n",
            "accuracy: 14.50, precision: 3.89, recall: 14.10, f1-score: 3.99\n",
            "VALIDATION:\n",
            "accuracy: 14.81, precision: 2.15, recall: 14.29, f1-score: 3.73\n",
            "LOSS train 2.0010517040888467 valid 1.9696104923884075\n",
            "EPOCH 12/30:\n",
            "TRAIN:\n",
            "accuracy: 14.69, precision: 21.15, recall: 14.30, f1-score: 4.39\n",
            "VALIDATION:\n",
            "accuracy: 17.04, precision: 8.10, recall: 16.58, f1-score: 7.29\n",
            "LOSS train 1.9924112293455336 valid 1.9919990698496501\n",
            "EPOCH 13/30:\n",
            "TRAIN:\n",
            "accuracy: 15.25, precision: 17.49, recall: 14.91, f1-score: 7.26\n",
            "VALIDATION:\n",
            "accuracy: 16.30, precision: 6.73, recall: 16.15, f1-score: 8.59\n",
            "LOSS train 1.9967535204357572 valid 1.9188599189122517\n",
            "saved: /Users/cyber_m/Desktop/Neural Network/nn-project/training/results/TIM2024-10-03_16:35:13/model.pt\n",
            "EPOCH 14/30:\n",
            "TRAIN:\n",
            "accuracy: 14.31, precision: 12.93, recall: 14.05, f1-score: 9.17\n",
            "VALIDATION:\n",
            "accuracy: 14.07, precision: 6.30, recall: 14.46, f1-score: 7.44\n",
            "LOSS train 1.9716132879257202 valid 1.9394545157750447\n",
            "EPOCH 15/30:\n",
            "TRAIN:\n",
            "accuracy: 14.31, precision: 13.93, recall: 14.11, f1-score: 6.69\n",
            "VALIDATION:\n",
            "accuracy: 14.81, precision: 6.83, recall: 15.08, f1-score: 4.98\n",
            "LOSS train 1.9876442220475938 valid 1.9717162052790325\n",
            "EPOCH 16/30:\n",
            "TRAIN:\n",
            "accuracy: 14.31, precision: 18.02, recall: 14.12, f1-score: 4.97\n",
            "VALIDATION:\n",
            "accuracy: 14.07, precision: 2.04, recall: 14.29, f1-score: 3.57\n",
            "LOSS train 1.9993220700158014 valid 1.9873225688934326\n",
            "EPOCH 17/30:\n",
            "TRAIN:\n",
            "accuracy: 13.94, precision: 6.11, recall: 13.73, f1-score: 4.51\n",
            "VALIDATION:\n",
            "accuracy: 15.56, precision: 16.82, recall: 15.72, f1-score: 8.37\n",
            "LOSS train 1.9931586583455403 valid 2.02342689037323\n",
            "EPOCH 18/30:\n",
            "TRAIN:\n",
            "accuracy: 15.63, precision: 9.85, recall: 15.48, f1-score: 9.45\n",
            "VALIDATION:\n",
            "accuracy: 11.11, precision: 8.66, recall: 10.83, f1-score: 8.93\n",
            "LOSS train 1.9830855131149292 valid 1.9687520662943523\n",
            "EPOCH 19/30:\n",
            "TRAIN:\n",
            "accuracy: 14.69, precision: 15.72, recall: 14.63, f1-score: 11.06\n",
            "VALIDATION:\n",
            "accuracy: 22.96, precision: 18.11, recall: 22.34, f1-score: 16.36\n",
            "LOSS train 1.9837404489517212 valid 1.9441863298416138\n",
            "EPOCH 20/30:\n",
            "TRAIN:\n",
            "accuracy: 14.50, precision: 10.38, recall: 14.35, f1-score: 10.24\n",
            "VALIDATION:\n",
            "accuracy: 12.59, precision: 5.54, recall: 12.33, f1-score: 7.59\n",
            "LOSS train 1.963575734032525 valid 1.9844789902369182\n",
            "EPOCH 21/30:\n",
            "TRAIN:\n",
            "accuracy: 11.68, precision: 6.49, recall: 11.54, f1-score: 7.46\n",
            "VALIDATION:\n",
            "accuracy: 17.78, precision: 8.70, recall: 17.48, f1-score: 11.26\n",
            "LOSS train 2.0066143539216785 valid 1.9748911460240681\n",
            "EPOCH 22/30:\n",
            "TRAIN:\n",
            "accuracy: 14.50, precision: 12.82, recall: 14.33, f1-score: 9.77\n",
            "VALIDATION:\n",
            "accuracy: 17.04, precision: 7.38, recall: 16.77, f1-score: 9.70\n",
            "LOSS train 1.9720573425292969 valid 1.9228380123774211\n",
            "EPOCH 23/30:\n",
            "TRAIN:\n",
            "accuracy: 15.25, precision: 13.53, recall: 15.08, f1-score: 10.02\n",
            "VALIDATION:\n",
            "accuracy: 16.30, precision: 10.77, recall: 16.02, f1-score: 11.49\n",
            "LOSS train 1.9772333171632555 valid 1.9530219634373982\n",
            "EPOCH 24/30:\n",
            "TRAIN:\n",
            "accuracy: 15.44, precision: 16.62, recall: 15.27, f1-score: 11.72\n",
            "VALIDATION:\n",
            "accuracy: 10.37, precision: 10.36, recall: 10.19, f1-score: 7.39\n",
            "LOSS train 1.9690561824374728 valid 1.9722853501637776\n",
            "EPOCH 25/30:\n",
            "TRAIN:\n",
            "accuracy: 12.62, precision: 12.21, recall: 12.47, f1-score: 9.49\n",
            "VALIDATION:\n",
            "accuracy: 15.56, precision: 16.73, recall: 15.30, f1-score: 10.84\n",
            "LOSS train 1.972562591234843 valid 1.9765135447184246\n",
            "EPOCH 26/30:\n",
            "TRAIN:\n",
            "accuracy: 15.25, precision: 20.56, recall: 15.08, f1-score: 10.41\n",
            "VALIDATION:\n",
            "accuracy: 11.11, precision: 3.62, recall: 10.90, f1-score: 5.41\n",
            "LOSS train 1.9607296917173598 valid 2.016495704650879\n",
            "EPOCH 27/30:\n",
            "TRAIN:\n",
            "accuracy: 16.95, precision: 14.69, recall: 16.76, f1-score: 12.55\n",
            "VALIDATION:\n",
            "accuracy: 13.33, precision: 5.99, recall: 12.97, f1-score: 6.64\n",
            "LOSS train 1.9527657826741536 valid 1.9769508043924968\n",
            "EPOCH 28/30:\n",
            "TRAIN:\n",
            "accuracy: 14.31, precision: 10.23, recall: 14.13, f1-score: 8.90\n",
            "VALIDATION:\n",
            "accuracy: 12.59, precision: 19.69, recall: 12.30, f1-score: 8.03\n",
            "LOSS train 1.9691409667332966 valid 1.934821327527364\n",
            "EPOCH 29/30:\n",
            "TRAIN:\n",
            "accuracy: 13.18, precision: 13.83, recall: 13.05, f1-score: 9.76\n",
            "VALIDATION:\n",
            "accuracy: 15.56, precision: 14.82, recall: 15.27, f1-score: 13.04\n",
            "LOSS train 1.974814282523261 valid 1.93526029586792\n",
            "EPOCH 30/30:\n",
            "TRAIN:\n",
            "accuracy: 13.94, precision: 11.66, recall: 13.77, f1-score: 10.30\n",
            "VALIDATION:\n",
            "accuracy: 11.85, precision: 13.55, recall: 11.50, f1-score: 7.89\n",
            "LOSS train 1.9803237782584295 valid 2.0075292189915976\n",
            "Metrics saved: /Users/cyber_m/Desktop/Neural Network/nn-project/training/results/TIM2024-10-03_16:35:13/training_metrics.json\n",
            "Metrics saved: /Users/cyber_m/Desktop/Neural Network/nn-project/training/results/TIM2024-10-03_16:35:13/validation_metrics.json\n",
            "Hydra configuration saved to /Users/cyber_m/Desktop/Neural Network/nn-project/training/results/TIM2024-10-03_16:35:13/parameters.txt\n"
          ]
        }
      ],
      "source": [
        "# Load the hyperparameters for training\n",
        "EPOCHS = cfg.train.num_epochs\n",
        "BATCH_SIZE = cfg.train.batch_size\n",
        "LEARNING_RATE = cfg.train.learning_rate\n",
        "OPTI = cfg.train.optimizer\n",
        "EARLY_STOPPING = cfg.train.early_stopping\n",
        "PATIENCE = cfg.train.patience\n",
        "LR_SCHEDULER = cfg.train.lr_scheduler\n",
        "LABEL_SMOOTHING = cfg.train.label_smoothing\n",
        "\n",
        "# Initialization of the metrics' dictionary\n",
        "training_metrics_dict = {\n",
        "    \"model\" : [model.model_name],\n",
        "    \"epoch\": [],\n",
        "    \"loss\": [],\n",
        "    \"accuracy\": [],\n",
        "    \"recall\": [],\n",
        "    \"precision\": [],\n",
        "    \"f1_score\": [],\n",
        "}\n",
        "validation_metrics_dict = {\n",
        "    \"model\" : [model.model_name],\n",
        "    \"epoch\": [],\n",
        "    \"loss\": [],\n",
        "    \"accuracy\": [],\n",
        "    \"recall\": [],\n",
        "    \"precision\": [],\n",
        "    \"f1_score\": [],\n",
        "}\n",
        "\n",
        "# Dataloader initialization for training, validation and test\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Definition of the Categorical Cross Entropy Loss\n",
        "loss_fn = nn.CrossEntropyLoss()#label_smoothing = LABEL_SMOOTHING)\n",
        "\n",
        "# Definition of the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=LR_SCHEDULER, patience=5)\n",
        "\n",
        "train(\n",
        "    EPOCHS, \n",
        "    training_metrics_dict, \n",
        "    validation_metrics_dict, \n",
        "    train_dataloader, \n",
        "    val_dataloader, \n",
        "    model, \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    lr_scheduler, \n",
        "    cfg\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz43RZa3iuRu"
      },
      "source": [
        "# TESTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etj9SlVBiuRu"
      },
      "outputs": [],
      "source": [
        "# Path to the model.pt\n",
        "model_path = './testing/model.pt'\n",
        "\n",
        "# Batch size of the dataloader\n",
        "BATCH_SIZE = cfg.train.batch_size\n",
        "\n",
        "# Initialization of the dataloader\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Loss Function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialization of the test metrics dictionary\n",
        "test_metrics_dict = {\n",
        "    \"model\" : [model.model_name],\n",
        "    \"epoch\": [],\n",
        "    \"loss\": [],\n",
        "    \"accuracy\": [],\n",
        "    \"recall\": [],\n",
        "    \"precision\": [],\n",
        "    \"f1_score\": [],\n",
        "}\n",
        "\n",
        "cm = test(\n",
        "    model,\n",
        "    model_path,\n",
        "    test_dataloader,\n",
        "    test_metrics_dict,\n",
        "    loss_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivuBD21_iuRu"
      },
      "source": [
        "# Loss and Accuracy (Training and Validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjiZ1_DTiuRu"
      },
      "outputs": [],
      "source": [
        "actual_dir = './training/results/TIM2024-10-03_13:49:30'\n",
        "# TO DO add an intermidiate path with the directories of best models (1 for each possibility)\n",
        "path_training_metrics = actual_dir + '/training_metrics.json'\n",
        "path_validation_metrics = actual_dir+ '/validation_metrics.json'\n",
        "\n",
        "# load the dictionary {'model': [model_name], 'epoch': [1, 2 ...], 'loss': [1.9, 1.8 ...], 'accuracy': [0.6, 0.7, ...], 'recall': [0.2, 0.3, ...], 'precision': [0.3, 0.4, ...], 'f1-score': [0.5, 0.6, ...]}\n",
        "data_training = utils.load_metrics(path_training_metrics)\n",
        "data_validation = utils.load_metrics(path_validation_metrics)\n",
        "\n",
        "# Load the epochs, loss and accuracy in training and validation for the plots\n",
        "epochs = data_training['epoch']\n",
        "training_loss = data_training['loss']\n",
        "training_accuracy = data_training['accuracy']\n",
        "\n",
        "validation_loss = data_validation['loss']\n",
        "validation_accuracy = data_validation['accuracy']\n",
        "\n",
        "# Plot section\n",
        "# 1st Subplot => Loss in Training and Validation\n",
        "# 2nd Subplot => Accuracy in Training and Validation\n",
        "utils.plot_loss_acc(epochs, training_loss, validation_loss, training_accuracy, validation_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaPMYl_ViuRu"
      },
      "source": [
        "# Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oT4TIk1BiuRu"
      },
      "outputs": [],
      "source": [
        "# Plot the confusion Matrix\n",
        "\n",
        "class_names = ['dis', 'gio', 'neu', 'pau', 'rab', 'sor', 'tri']\n",
        "utils.plot_confusion_matrix(cm, class_names, cmap='rocket')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
