{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dIRawMyjYAz"
      },
      "source": [
        "⚠️Clone the repository to use all the functions needed by the main code⚠️\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvEJSRQzjVVO",
        "outputId": "621797ba-2595-461d-e6ec-8c898391b5dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are running locally!\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    mode = 'colab'\n",
        "except:\n",
        "    mode = 'local'\n",
        "\n",
        "if mode == 'colab':\n",
        "    !git clone \"https://github.com/cybernetic-m/nn-project.git\"\n",
        "    !pip install hydra-core --upgrade convkan\n",
        "else:\n",
        "    print(\"You are running locally!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZNKzy8qbbu2"
      },
      "source": [
        "# IMPORT AND INITIALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dKakfnMRbKum"
      },
      "outputs": [],
      "source": [
        "# Import for config.yaml file\n",
        "from hydra import initialize, compose\n",
        "from hydra.core.global_hydra import GlobalHydra\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import Audio\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "if mode == 'colab':\n",
        "    sys.path.append('/content/nn-project/dataloader')\n",
        "    sys.path.append('/content/nn-project/model')\n",
        "    sys.path.append('/content/nn-project/module')\n",
        "    sys.path.append('/content/nn-project/training')\n",
        "    sys.path.append('/content/nn-project/testing')\n",
        "    from preprocessing import Preprocessing\n",
        "    import utils\n",
        "    import dataset\n",
        "    from ctim import CTIM\n",
        "    from train import *\n",
        "    from test import *\n",
        "\n",
        "\n",
        "if mode == 'local':\n",
        "    import dataloader.utils as utils\n",
        "    import dataloader.dataset as dataset\n",
        "    from dataloader.preprocessing import Preprocessing\n",
        "    from model.ctim import CTIM\n",
        "    from training.train import train as train\n",
        "    from testing.test import test as test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzpebiXBPJE-"
      },
      "source": [
        "Reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "X3f4K2m8PIrc"
      },
      "outputs": [],
      "source": [
        "# Set the seed\n",
        "seed = 46\n",
        "\n",
        "# Set seed for torch, numpy and random libraries\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# Set the devide mode on GPU (if available CUDA for Nvidia and  MPS for Apple Silicon) or CPU\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mV42Zq-UKCz"
      },
      "source": [
        "## Download and Split Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV_poPYiccs2",
        "outputId": "e50c5a1d-7044-4167-99c3-48ec2f05c711"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset already downloaded\n",
            "Dataset already unzipped\n"
          ]
        }
      ],
      "source": [
        "link_dataset = \"https://drive.google.com/file/d/1nzKBta2M3khw7Ql_S7atYg-H-bWDiOxr/view?usp=drive_link\"\n",
        "gdrive_link = \"https://drive.google.com/uc?export=download&id=\"\n",
        "\n",
        "if mode == 'colab':\n",
        "    destination_dir = \"/content/emovo.zip\"\n",
        "    extract_dir = '/content/dataset'\n",
        "    emovo_dir = '/content/dataset/EMOVO'\n",
        "    emovo_split_dir = '/content/dataset/EMOVO_split'\n",
        "elif mode == 'local':\n",
        "    destination_dir = \"./emovo.zip\"\n",
        "    extract_dir = \"./dataset\"\n",
        "    emovo_dir = \"./dataset/EMOVO\"\n",
        "    emovo_split_dir = './dataset/EMOVO_split'\n",
        "\n",
        "utils.download_dataset(link_dataset, destination_dir, gdrive_link, extract_dir)\n",
        "\n",
        "utils.dataset_split(emovo_dir, extract_dir, 0.7, 0.2, 0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "utils.dataset_split_mfcc('./EMOVO.npy', extract_dir, 0.7, 0.2, 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrZ8n_tp2nfh"
      },
      "source": [
        "# Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBRzIeeY2nfh",
        "outputId": "b7a2bfe4-c0e0-40ab-94c7-f9a3afffa6d9"
      },
      "outputs": [],
      "source": [
        "preprocessing = Preprocessing(device=device)\n",
        "\n",
        "utils.augment_data(emovo_split_dir, extract_dir, preprocessing, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVRKvN26UUq9"
      },
      "source": [
        "## Dataset Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRn6VibgUXm9",
        "outputId": "491fc476-55ab-423a-f653-619cae3944dc"
      },
      "outputs": [],
      "source": [
        "train_path = os.path.join(extract_dir, 'EMOVO_split', 'train')\n",
        "test_path = os.path.join(extract_dir, 'EMOVO_split', 'test')\n",
        "val_path = os.path.join(extract_dir, 'EMOVO_split', 'val')\n",
        "\n",
        "train_dataset = dataset.EMOVO_Dataset(train_path, False, device=device)\n",
        "test_dataset = dataset.EMOVO_Dataset(test_path, False, device=device)\n",
        "val_dataset = dataset.EMOVO_Dataset(val_path, False, device=device)\n",
        "\n",
        "train_aug_path = os.path.join(extract_dir, 'EMOVO_aug', 'train')\n",
        "test_aug_path = os.path.join(extract_dir, 'EMOVO_aug', 'test')\n",
        "val_aug_path = os.path.join(extract_dir, 'EMOVO_aug', 'val')\n",
        "\n",
        "train_aug_dataset = dataset.EMOVO_Dataset(train_aug_path, device=device)\n",
        "test_aug_dataset = dataset.EMOVO_Dataset(test_aug_path, device=device)\n",
        "val_aug_dataset = dataset.EMOVO_Dataset(val_aug_path, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ9URMKkfQmU"
      },
      "source": [
        "Example of audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "mcblPPkeZw35",
        "outputId": "f79994e1-f311-4cb7-f112-de385323b3ae"
      },
      "outputs": [],
      "source": [
        "classes = ['dis', 'gio', 'neu', 'pau', 'rab', 'sor', 'tri']\n",
        "random_num = random.randint(1, train_dataset.__len__())\n",
        "data, label = train_dataset[random_num]\n",
        "\n",
        "print(\"Tensor of shape:\",data[0].shape, \"Sample Rate:\", data[1])\n",
        "print(\"Class:\", classes[label], \"\\n\")\n",
        "\n",
        "# Waveform\n",
        "# Compute the average of both channels to get a mono waveform\n",
        "mono_waveform = data[0].mean(dim=0)\n",
        "num_samples = mono_waveform.shape[0]\n",
        "sample_rate = data[1]\n",
        "time_axis = np.linspace(0, num_samples / sample_rate, num_samples)\n",
        "\n",
        "# Play the audio\n",
        "display(Audio(mono_waveform.cpu().numpy(), rate=sample_rate))\n",
        "\n",
        "# Plot the averaged (mono) waveform\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(time_axis, mono_waveform.cpu().numpy())\n",
        "plt.title('Waveform (Averaged Mono)')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSzFkhBu2nfi"
      },
      "source": [
        "Example of augmented audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5XiV4x82nfi"
      },
      "outputs": [],
      "source": [
        "# TO DO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnwkpxVjIDni"
      },
      "source": [
        "Data distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "q3Hs-83fIHPT",
        "outputId": "f64ac95c-b03e-4b4b-9156-c7e93f23e3b8"
      },
      "outputs": [],
      "source": [
        "train_counts = train_dataset.get_info()\n",
        "test_counts = test_dataset.get_info()\n",
        "val_counts = val_dataset.get_info()\n",
        "classes = ['dis', 'gio', 'neu', 'pau', 'rab', 'sor', 'tri']\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1,3, figsize=(16, 4))\n",
        "\n",
        "ax[0].bar(classes, train_counts, color='limegreen')\n",
        "ax[0].set_title(\"Data distribution in Training Set\")\n",
        "ax[0].set_xlabel(\"Classes\")\n",
        "ax[0].set_xticks(ticks=range(7), labels=classes)\n",
        "ax[0].set_ylabel(\"Number of samples\")\n",
        "\n",
        "ax[1].bar(classes, test_counts, color='seagreen')\n",
        "ax[1].set_title(\"Data distribution in Test Set\")\n",
        "ax[1].set_xlabel(\"Classes\")\n",
        "ax[1].set_xticks(ticks=range(7), labels=classes)\n",
        "ax[1].set_ylabel(\"Number of samples\")\n",
        "\n",
        "ax[2].bar(classes, val_counts, color='olive')\n",
        "ax[2].set_title(\"Data distribution in Validation Set\")\n",
        "ax[2].set_xlabel(\"Classes\")\n",
        "ax[2].set_xticks(ticks=range(7), labels=classes)\n",
        "ax[2].set_ylabel(\"Number of samples\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzyjYYHp2nfi"
      },
      "source": [
        "Data augmented distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "WUqtdsOf2nfi",
        "outputId": "3d60a842-b1f0-41c5-f68d-d605dbfcbd14"
      },
      "outputs": [],
      "source": [
        "train_aug_counts = train_aug_dataset.get_info()\n",
        "test_aug_counts = test_aug_dataset.get_info()\n",
        "val_aug_counts = val_aug_dataset.get_info()\n",
        "classes = ['dis', 'gio', 'neu', 'pau', 'rab', 'sor', 'tri']\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1,3, figsize=(16, 4))\n",
        "\n",
        "ax[0].bar(classes, train_aug_counts, color='limegreen')\n",
        "ax[0].set_title(\"Data distribution in Training Set\")\n",
        "ax[0].set_xlabel(\"Classes\")\n",
        "ax[0].set_xticks(ticks=range(7), labels=classes)\n",
        "ax[0].set_ylabel(\"Number of samples\")\n",
        "\n",
        "ax[1].bar(classes, test_aug_counts, color='seagreen')\n",
        "ax[1].set_title(\"Data distribution in Test Set\")\n",
        "ax[1].set_xlabel(\"Classes\")\n",
        "ax[1].set_xticks(ticks=range(7), labels=classes)\n",
        "ax[1].set_ylabel(\"Number of samples\")\n",
        "\n",
        "ax[2].bar(classes, val_aug_counts, color='olive')\n",
        "ax[2].set_title(\"Data distribution in Validation Set\")\n",
        "ax[2].set_xlabel(\"Classes\")\n",
        "ax[2].set_xticks(ticks=range(7), labels=classes)\n",
        "ax[2].set_ylabel(\"Number of samples\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6u2yOd0iuRt"
      },
      "source": [
        "# HYDRA INITIALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LK9-Zzs-iuRt",
        "outputId": "e32700b7-b64a-4601-cd21-6b343b4a1365"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hydra now initialized!\n"
          ]
        }
      ],
      "source": [
        "# Check if Hydra is initialized\n",
        "if GlobalHydra().is_initialized():\n",
        "    # Clear the Hydra instance if it is initialized\n",
        "    GlobalHydra.instance().clear()\n",
        "    print(\"Hydra instance was initialized and has been cleared.\")\n",
        "else:\n",
        "    # Initialize\n",
        "    print(\"Hydra now initialized!\")\n",
        "\n",
        "# Initialization and Load configuration\n",
        "if mode == 'local':\n",
        "  initialize(config_path=\"./conf\", job_name=\"notebook_nn_exam\", version_base=None)\n",
        "  cfg = compose(config_name=\"config\")\n",
        "\n",
        "elif mode == 'colab':\n",
        "  initialize(config_path=\"./nn-project/conf\", job_name=\"notebook_nn_exam\", version_base=None)\n",
        "  cfg = compose(config_name=\"config\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Vn5O9hp_iuRt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------+------------+\n",
            "|                       Modules                       | Parameters |\n",
            "+-----------------------------------------------------+------------+\n",
            "|               ctim_net.weightsdyn_fun               |     8      |\n",
            "|             ctim_net.conv_forward.weight            |    1521    |\n",
            "|              ctim_net.conv_forward.bias             |     39     |\n",
            "|             ctim_net.conv_reverse.weight            |    1521    |\n",
            "|              ctim_net.conv_reverse.bias             |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.0.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.0.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.0.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.0.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.0.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.0.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.0.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.0.batch_norm2.bias  |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.1.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.1.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.1.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.1.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.1.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.1.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.1.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.1.batch_norm2.bias  |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.2.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.2.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.2.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.2.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.2.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.2.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.2.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.2.batch_norm2.bias  |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.3.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.3.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.3.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.3.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.3.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.3.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.3.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.3.batch_norm2.bias  |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.4.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.4.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.4.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.4.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.4.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.4.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.4.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.4.batch_norm2.bias  |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.5.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.5.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.5.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.5.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.5.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.5.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.5.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.5.batch_norm2.bias  |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.6.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.6.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.6.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.6.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.6.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.6.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.6.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.6.batch_norm2.bias  |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.7.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.7.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_forward.7.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_forward.7.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.7.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.7.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_forward.7.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_forward.7.batch_norm2.bias  |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.0.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.0.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.0.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.0.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.0.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.0.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.0.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.0.batch_norm2.bias  |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.1.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.1.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.1.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.1.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.1.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.1.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.1.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.1.batch_norm2.bias  |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.2.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.2.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.2.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.2.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.2.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.2.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.2.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.2.batch_norm2.bias  |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.3.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.3.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.3.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.3.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.3.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.3.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.3.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.3.batch_norm2.bias  |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.4.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.4.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.4.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.4.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.4.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.4.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.4.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.4.batch_norm2.bias  |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.5.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.5.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.5.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.5.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.5.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.5.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.5.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.5.batch_norm2.bias  |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.6.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.6.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.6.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.6.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.6.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.6.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.6.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.6.batch_norm2.bias  |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.7.conv1.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.7.conv1.bias     |     39     |\n",
            "|    ctim_net.TempAw_Blocks_reverse.7.conv2.weight    |    3042    |\n",
            "|     ctim_net.TempAw_Blocks_reverse.7.conv2.bias     |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.7.batch_norm1.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.7.batch_norm1.bias  |     39     |\n",
            "| ctim_net.TempAw_Blocks_reverse.7.batch_norm2.weight |     39     |\n",
            "|  ctim_net.TempAw_Blocks_reverse.7.batch_norm2.bias  |     39     |\n",
            "|                  classifier.weight                  |    273     |\n",
            "|                   classifier.bias                   |     7      |\n",
            "+-----------------------------------------------------+------------+\n",
            "Total Trainable Params: 104496\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "104496"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the hyperparameters of the model\n",
        "DROPOUT_RATE = cfg.model.dropout_rate\n",
        "KERNEL_SIZE = cfg.model.kernel_size\n",
        "N_TAB = cfg.model.n_temporal_aware_block\n",
        "N_FILTER = cfg.model.n_filter\n",
        "NUM_FEATURES = cfg.model.num_features\n",
        "CK = cfg.model.ck\n",
        "USE_KAN = cfg.model.use_kan\n",
        "OMEGA_0 = cfg.model.omega_0\n",
        "IS_SIREN = cfg.model.is_siren\n",
        "\n",
        "model = CTIM(\n",
        "    kernel_size=KERNEL_SIZE,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    n_temporal_aware_block=N_TAB,\n",
        "    n_filter=N_FILTER,\n",
        "    in_channels=39,\n",
        "    ck=CK,\n",
        "    num_features=NUM_FEATURES,\n",
        "    num_classes = 7,\n",
        "    use_kan = False,\n",
        "    omega_0=OMEGA_0,\n",
        "    is_siren=IS_SIREN,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "def count_parameters(model):\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad:\n",
        "            continue\n",
        "        params = parameter.numel()\n",
        "        table.add_row([name, params])\n",
        "        total_params += params\n",
        "    print(table)\n",
        "    print(f\"Total Trainable Params: {total_params}\")\n",
        "    return total_params\n",
        "    \n",
        "count_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUdCPiRZiuRu"
      },
      "source": [
        "# TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_path = os.path.join(extract_dir, 'EMOVO_split_MFCC', 'train')\n",
        "test_path = os.path.join(extract_dir, 'EMOVO_split_MFCC', 'test')\n",
        "val_path = os.path.join(extract_dir, 'EMOVO_split_MFCC', 'val')\n",
        "\n",
        "\n",
        "# \n",
        "train_dataset = dataset.EMOVO_Dataset(train_path, feature_extract=False, mfcc_np=True, device=device)\n",
        "test_dataset = dataset.EMOVO_Dataset(test_path, feature_extract=False, mfcc_np=True, device=device)\n",
        "val_dataset = dataset.EMOVO_Dataset(val_path, feature_extract=False, mfcc_np=True, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EmPUS-iBiuRu",
        "outputId": "58030685-a642-4cea-ff80-09af8c680bac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 1/500:\n",
            "TRAIN:\n",
            "accuracy: 13.18, precision: 7.54, recall: 13.33, f1-score: 9.61\n",
            "VALIDATION:\n",
            "accuracy: 20.00, precision: 29.08, recall: 20.83, f1-score: 15.49\n",
            "LOSS train 13.287283208635118 valid 3.4637120564778647\n",
            "saved: /Users/cyber_m/Desktop/Neural Network/nn-project/training/results/TIM2024-10-03_17:17:47/model.pt\n",
            "EPOCH 2/500:\n",
            "TRAIN:\n",
            "accuracy: 19.40, precision: 18.33, recall: 19.32, f1-score: 17.03\n",
            "VALIDATION:\n",
            "accuracy: 20.74, precision: 13.42, recall: 20.04, f1-score: 10.71\n",
            "LOSS train 4.366836309432983 valid 4.680892149607341\n",
            "EPOCH 3/500:\n",
            "TRAIN:\n",
            "accuracy: 20.15, precision: 31.32, recall: 20.23, f1-score: 20.19\n",
            "VALIDATION:\n",
            "accuracy: 21.48, precision: 27.48, recall: 22.66, f1-score: 14.69\n",
            "LOSS train 3.3371951315138073 valid 2.773470481236776\n",
            "saved: /Users/cyber_m/Desktop/Neural Network/nn-project/training/results/TIM2024-10-03_17:17:53/model.pt\n",
            "EPOCH 4/500:\n",
            "TRAIN:\n",
            "accuracy: 29.19, precision: 28.47, recall: 29.36, f1-score: 27.10\n",
            "VALIDATION:\n",
            "accuracy: 31.85, precision: 25.83, recall: 32.16, f1-score: 26.83\n",
            "LOSS train 2.6734792391459146 valid 2.486367623011271\n",
            "saved: /Users/cyber_m/Desktop/Neural Network/nn-project/training/results/TIM2024-10-03_17:17:56/model.pt\n",
            "EPOCH 5/500:\n",
            "TRAIN:\n",
            "accuracy: 35.22, precision: 35.42, recall: 35.37, f1-score: 34.96\n",
            "VALIDATION:\n",
            "accuracy: 33.33, precision: 34.55, recall: 34.39, f1-score: 30.27\n",
            "LOSS train 2.129779961374071 valid 2.0458397467931113\n",
            "saved: /Users/cyber_m/Desktop/Neural Network/nn-project/training/results/TIM2024-10-03_17:18:00/model.pt\n",
            "EPOCH 6/500:\n",
            "TRAIN:\n",
            "accuracy: 36.35, precision: 38.75, recall: 36.34, f1-score: 36.87\n",
            "VALIDATION:\n",
            "accuracy: 46.67, precision: 55.16, recall: 46.53, f1-score: 44.87\n",
            "LOSS train 2.0112567212846546 valid 1.4428326686223347\n",
            "saved: /Users/cyber_m/Desktop/Neural Network/nn-project/training/results/TIM2024-10-03_17:18:03/model.pt\n",
            "EPOCH 7/500:\n",
            "TRAIN:\n",
            "accuracy: 36.16, precision: 36.35, recall: 36.35, f1-score: 35.42\n",
            "VALIDATION:\n",
            "accuracy: 42.96, precision: 57.95, recall: 42.80, f1-score: 42.15\n",
            "LOSS train 1.9500902096430461 valid 1.6292719443639119\n",
            "EPOCH 8/500:\n",
            "TRAIN:\n",
            "accuracy: 39.55, precision: 43.74, recall: 39.56, f1-score: 39.91\n",
            "VALIDATION:\n",
            "accuracy: 58.52, precision: 68.40, recall: 59.02, f1-score: 57.48\n",
            "LOSS train 1.8072348833084106 valid 1.280343770980835\n",
            "saved: /Users/cyber_m/Desktop/Neural Network/nn-project/training/results/TIM2024-10-03_17:18:10/model.pt\n",
            "EPOCH 9/500:\n",
            "TRAIN:\n",
            "accuracy: 40.11, precision: 41.41, recall: 40.31, f1-score: 39.34\n",
            "VALIDATION:\n",
            "accuracy: 42.96, precision: 69.40, recall: 42.63, f1-score: 39.67\n",
            "LOSS train 1.8006009525722928 valid 1.6990065972010295\n",
            "EPOCH 10/500:\n",
            "TRAIN:\n",
            "accuracy: 45.76, precision: 48.65, recall: 45.70, f1-score: 45.82\n",
            "VALIDATION:\n",
            "accuracy: 42.22, precision: 59.56, recall: 42.06, f1-score: 40.38\n",
            "LOSS train 1.6866333882013957 valid 1.985764503479004\n",
            "EPOCH 11/500:\n",
            "TRAIN:\n",
            "accuracy: 45.95, precision: 47.39, recall: 46.19, f1-score: 45.90\n",
            "VALIDATION:\n",
            "accuracy: 60.74, precision: 69.66, recall: 60.52, f1-score: 60.20\n",
            "LOSS train 1.6364321046405368 valid 1.2630359331766765\n",
            "saved: /Users/cyber_m/Desktop/Neural Network/nn-project/training/results/TIM2024-10-03_17:18:18/model.pt\n",
            "EPOCH 12/500:\n",
            "TRAIN:\n",
            "accuracy: 47.46, precision: 48.42, recall: 47.55, f1-score: 47.58\n",
            "VALIDATION:\n",
            "accuracy: 55.56, precision: 68.21, recall: 56.09, f1-score: 52.00\n",
            "LOSS train 1.5459171798494127 valid 1.3844703038533528\n",
            "EPOCH 13/500:\n",
            "TRAIN:\n",
            "accuracy: 51.41, precision: 51.00, recall: 51.62, f1-score: 51.17\n",
            "VALIDATION:\n",
            "accuracy: 58.52, precision: 63.46, recall: 58.94, f1-score: 58.08\n",
            "LOSS train 1.4569712214999728 valid 1.3370303710301716\n",
            "EPOCH 14/500:\n",
            "TRAIN:\n",
            "accuracy: 50.66, precision: 51.25, recall: 50.72, f1-score: 50.47\n",
            "VALIDATION:\n",
            "accuracy: 63.70, precision: 73.63, recall: 63.48, f1-score: 62.11\n",
            "LOSS train 1.5088752905527751 valid 1.289078712463379\n",
            "EPOCH 15/500:\n",
            "TRAIN:\n",
            "accuracy: 51.79, precision: 54.22, recall: 51.79, f1-score: 52.19\n",
            "VALIDATION:\n",
            "accuracy: 68.15, precision: 75.42, recall: 68.22, f1-score: 66.49\n",
            "LOSS train 1.5191060569551256 valid 1.1733254194259644\n",
            "saved: /Users/cyber_m/Desktop/Neural Network/nn-project/training/results/TIM2024-10-03_17:18:30/model.pt\n",
            "EPOCH 16/500:\n",
            "TRAIN:\n",
            "accuracy: 54.99, precision: 55.61, recall: 55.16, f1-score: 54.93\n",
            "VALIDATION:\n",
            "accuracy: 65.19, precision: 81.24, recall: 65.13, f1-score: 65.70\n",
            "LOSS train 1.3927687406539917 valid 1.1136085987091064\n",
            "saved: /Users/cyber_m/Desktop/Neural Network/nn-project/training/results/TIM2024-10-03_17:18:34/model.pt\n",
            "EPOCH 17/500:\n",
            "TRAIN:\n",
            "accuracy: 56.50, precision: 57.15, recall: 56.59, f1-score: 56.08\n",
            "VALIDATION:\n",
            "accuracy: 68.15, precision: 79.21, recall: 68.07, f1-score: 69.96\n",
            "LOSS train 1.4163200987709894 valid 1.4032524824142456\n",
            "EPOCH 18/500:\n",
            "TRAIN:\n",
            "accuracy: 61.02, precision: 62.10, recall: 61.08, f1-score: 61.21\n",
            "VALIDATION:\n",
            "accuracy: 71.11, precision: 77.93, recall: 70.84, f1-score: 71.02\n",
            "LOSS train 1.3503956927193537 valid 1.0327130357424419\n",
            "saved: /Users/cyber_m/Desktop/Neural Network/nn-project/training/results/TIM2024-10-03_17:18:40/model.pt\n",
            "EPOCH 19/500:\n",
            "TRAIN:\n",
            "accuracy: 56.31, precision: 58.08, recall: 56.43, f1-score: 56.60\n",
            "VALIDATION:\n",
            "accuracy: 52.59, precision: 55.73, recall: 53.38, f1-score: 47.18\n",
            "LOSS train 1.3737481435139973 valid 1.74018394947052\n",
            "EPOCH 20/500:\n",
            "TRAIN:\n",
            "accuracy: 58.57, precision: 60.37, recall: 58.66, f1-score: 58.81\n",
            "VALIDATION:\n",
            "accuracy: 71.85, precision: 79.26, recall: 71.71, f1-score: 72.10\n",
            "LOSS train 1.3136290576722887 valid 1.0740430355072021\n",
            "EPOCH 21/500:\n",
            "TRAIN:\n",
            "accuracy: 61.96, precision: 63.42, recall: 61.96, f1-score: 61.89\n",
            "VALIDATION:\n",
            "accuracy: 75.56, precision: 77.91, recall: 75.68, f1-score: 75.14\n",
            "LOSS train 1.322627239757114 valid 1.102403958638509\n",
            "EPOCH 22/500:\n",
            "TRAIN:\n",
            "accuracy: 63.65, precision: 65.50, recall: 63.84, f1-score: 63.54\n",
            "VALIDATION:\n",
            "accuracy: 68.89, precision: 79.42, recall: 69.06, f1-score: 68.73\n",
            "LOSS train 1.2566088570488825 valid 1.2124160528182983\n",
            "EPOCH 23/500:\n",
            "TRAIN:\n",
            "accuracy: 63.28, precision: 63.59, recall: 63.34, f1-score: 63.20\n",
            "VALIDATION:\n",
            "accuracy: 70.37, precision: 78.50, recall: 70.12, f1-score: 66.99\n",
            "LOSS train 1.304449650976393 valid 1.1376006205876668\n",
            "EPOCH 24/500:\n",
            "TRAIN:\n",
            "accuracy: 61.39, precision: 64.65, recall: 61.45, f1-score: 61.99\n",
            "VALIDATION:\n",
            "accuracy: 69.63, precision: 79.23, recall: 69.97, f1-score: 68.64\n",
            "LOSS train 1.2328305906719632 valid 1.0756832758585613\n",
            "EPOCH 25/500:\n",
            "TRAIN:\n",
            "accuracy: 68.17, precision: 71.87, recall: 68.39, f1-score: 67.25\n",
            "VALIDATION:\n",
            "accuracy: 82.96, precision: 86.38, recall: 83.10, f1-score: 83.42\n",
            "LOSS train 1.175399402777354 valid 0.9143786231676737\n",
            "saved: /Users/cyber_m/Desktop/Neural Network/nn-project/training/results/TIM2024-10-03_17:19:00/model.pt\n",
            "EPOCH 26/500:\n",
            "TRAIN:\n",
            "accuracy: 70.62, precision: 71.41, recall: 70.69, f1-score: 70.79\n",
            "VALIDATION:\n",
            "accuracy: 80.74, precision: 85.31, recall: 80.91, f1-score: 80.44\n",
            "LOSS train 1.113465044233534 valid 0.986210823059082\n",
            "EPOCH 27/500:\n",
            "TRAIN:\n",
            "accuracy: 68.17, precision: 69.07, recall: 68.19, f1-score: 68.45\n",
            "VALIDATION:\n",
            "accuracy: 89.63, precision: 90.83, recall: 89.49, f1-score: 89.64\n",
            "LOSS train 1.1662934753629897 valid 0.867255171140035\n",
            "saved: /Users/cyber_m/Desktop/Neural Network/nn-project/training/results/TIM2024-10-03_17:19:07/model.pt\n",
            "EPOCH 28/500:\n",
            "TRAIN:\n",
            "accuracy: 71.37, precision: 71.45, recall: 71.53, f1-score: 71.21\n",
            "VALIDATION:\n",
            "accuracy: 88.89, precision: 89.10, recall: 88.85, f1-score: 88.85\n",
            "LOSS train 1.1115598016315036 valid 0.8543879787127177\n",
            "saved: /Users/cyber_m/Desktop/Neural Network/nn-project/training/results/TIM2024-10-03_17:19:11/model.pt\n",
            "EPOCH 29/500:\n",
            "TRAIN:\n",
            "accuracy: 68.74, precision: 69.19, recall: 68.80, f1-score: 68.86\n",
            "VALIDATION:\n",
            "accuracy: 88.89, precision: 89.73, recall: 88.77, f1-score: 88.86\n",
            "LOSS train 1.0996378130382962 valid 0.9071610967318217\n",
            "EPOCH 30/500:\n",
            "TRAIN:\n",
            "accuracy: 69.30, precision: 70.42, recall: 69.34, f1-score: 69.62\n",
            "VALIDATION:\n",
            "accuracy: 88.15, precision: 89.07, recall: 87.98, f1-score: 88.12\n",
            "LOSS train 1.1292304595311482 valid 0.849574605623881\n",
            "saved: /Users/cyber_m/Desktop/Neural Network/nn-project/training/results/TIM2024-10-03_17:19:21/model.pt\n",
            "EPOCH 31/500:\n",
            "TRAIN:\n",
            "accuracy: 72.88, precision: 73.41, recall: 72.98, f1-score: 73.09\n",
            "VALIDATION:\n",
            "accuracy: 89.63, precision: 90.90, recall: 89.64, f1-score: 89.87\n",
            "LOSS train 1.0818709399965074 valid 0.8758846124013265\n",
            "EPOCH 32/500:\n",
            "TRAIN:\n",
            "accuracy: 69.87, precision: 70.22, recall: 69.97, f1-score: 69.91\n",
            "VALIDATION:\n",
            "accuracy: 88.15, precision: 90.41, recall: 88.02, f1-score: 88.40\n",
            "LOSS train 1.1059620512856378 valid 0.8150213162104288\n",
            "saved: /Users/cyber_m/Desktop/Neural Network/nn-project/training/results/TIM2024-10-03_17:19:29/model.pt\n",
            "EPOCH 33/500:\n",
            "TRAIN:\n",
            "accuracy: 71.37, precision: 71.74, recall: 71.38, f1-score: 71.40\n",
            "VALIDATION:\n",
            "accuracy: 89.63, precision: 90.42, recall: 89.64, f1-score: 89.68\n",
            "LOSS train 1.0995033118459914 valid 0.9687520464261373\n",
            "EPOCH 34/500:\n",
            "TRAIN:\n",
            "accuracy: 74.01, precision: 74.28, recall: 74.10, f1-score: 74.16\n",
            "VALIDATION:\n",
            "accuracy: 89.63, precision: 90.39, recall: 89.57, f1-score: 89.53\n",
            "LOSS train 1.0839952958954706 valid 0.8534653981526693\n",
            "EPOCH 35/500:\n",
            "TRAIN:\n",
            "accuracy: 73.07, precision: 73.26, recall: 73.20, f1-score: 73.01\n",
            "VALIDATION:\n",
            "accuracy: 88.89, precision: 89.89, recall: 88.77, f1-score: 88.91\n",
            "LOSS train 1.0865752432081435 valid 0.8558142979939779\n",
            "EPOCH 36/500:\n",
            "TRAIN:\n",
            "accuracy: 70.62, precision: 71.31, recall: 70.72, f1-score: 70.60\n",
            "VALIDATION:\n",
            "accuracy: 88.15, precision: 89.14, recall: 88.06, f1-score: 88.18\n",
            "LOSS train 1.0979709227879841 valid 0.8652940988540649\n",
            "EPOCH 37/500:\n",
            "TRAIN:\n",
            "accuracy: 73.45, precision: 74.02, recall: 73.48, f1-score: 73.55\n",
            "VALIDATION:\n",
            "accuracy: 88.15, precision: 88.95, recall: 88.06, f1-score: 88.19\n",
            "LOSS train 1.0635788904296026 valid 0.9052555561065674\n",
            "EPOCH 38/500:\n",
            "TRAIN:\n",
            "accuracy: 70.06, precision: 70.35, recall: 70.18, f1-score: 69.99\n",
            "VALIDATION:\n",
            "accuracy: 90.37, precision: 91.45, recall: 90.28, f1-score: 90.48\n",
            "LOSS train 1.089496824476454 valid 0.7664540807406107\n",
            "saved: /Users/cyber_m/Desktop/Neural Network/nn-project/training/results/TIM2024-10-03_17:19:47/model.pt\n",
            "EPOCH 39/500:\n",
            "TRAIN:\n",
            "accuracy: 73.63, precision: 73.88, recall: 73.71, f1-score: 73.64\n",
            "VALIDATION:\n",
            "accuracy: 88.89, precision: 90.01, recall: 88.85, f1-score: 89.07\n",
            "LOSS train 1.094250241915385 valid 0.8088027040163676\n",
            "EPOCH 40/500:\n",
            "TRAIN:\n",
            "accuracy: 72.32, precision: 73.00, recall: 72.39, f1-score: 72.55\n",
            "VALIDATION:\n",
            "accuracy: 90.37, precision: 91.17, recall: 90.44, f1-score: 90.55\n",
            "LOSS train 1.086933970451355 valid 0.8445569276809692\n",
            "EPOCH 41/500:\n",
            "TRAIN:\n",
            "accuracy: 73.63, precision: 73.70, recall: 73.73, f1-score: 73.69\n",
            "VALIDATION:\n",
            "accuracy: 89.63, precision: 90.82, recall: 89.53, f1-score: 89.67\n",
            "LOSS train 1.0527747008535597 valid 0.8248354991277059\n",
            "EPOCH 42/500:\n",
            "TRAIN:\n",
            "accuracy: 72.50, precision: 74.39, recall: 72.54, f1-score: 72.97\n",
            "VALIDATION:\n",
            "accuracy: 89.63, precision: 90.59, recall: 89.57, f1-score: 89.58\n",
            "LOSS train 1.0537872976726956 valid 0.7790844241778055\n",
            "EPOCH 43/500:\n",
            "TRAIN:\n",
            "accuracy: 74.58, precision: 75.01, recall: 74.63, f1-score: 74.63\n",
            "VALIDATION:\n",
            "accuracy: 89.63, precision: 90.19, recall: 89.57, f1-score: 89.48\n",
            "LOSS train 1.0538407166798909 valid 0.7812028527259827\n",
            "EPOCH 44/500:\n",
            "TRAIN:\n",
            "accuracy: 71.37, precision: 71.37, recall: 71.53, f1-score: 71.29\n",
            "VALIDATION:\n",
            "accuracy: 89.63, precision: 90.82, recall: 89.52, f1-score: 89.62\n",
            "LOSS train 1.056091354952918 valid 0.8954604466756185\n",
            "EPOCH 45/500:\n",
            "TRAIN:\n",
            "accuracy: 76.08, precision: 76.43, recall: 76.15, f1-score: 76.20\n",
            "VALIDATION:\n",
            "accuracy: 89.63, precision: 90.52, recall: 89.52, f1-score: 89.59\n",
            "LOSS train 1.0196808642811246 valid 0.7783648570378622\n",
            "EPOCH 46/500:\n",
            "TRAIN:\n",
            "accuracy: 74.20, precision: 74.59, recall: 74.26, f1-score: 74.28\n",
            "VALIDATION:\n",
            "accuracy: 88.89, precision: 89.89, recall: 88.77, f1-score: 88.91\n",
            "LOSS train 1.0200014180607266 valid 0.7888648509979248\n",
            "EPOCH 47/500:\n",
            "TRAIN:\n",
            "accuracy: 77.02, precision: 77.37, recall: 77.12, f1-score: 77.16\n",
            "VALIDATION:\n",
            "accuracy: 89.63, precision: 90.44, recall: 89.57, f1-score: 89.60\n",
            "LOSS train 1.014768832259708 valid 0.7572351892789205\n",
            "saved: /Users/cyber_m/Desktop/Neural Network/nn-project/training/results/TIM2024-10-03_17:20:13/model.pt\n",
            "EPOCH 48/500:\n",
            "TRAIN:\n",
            "accuracy: 74.20, precision: 74.51, recall: 74.24, f1-score: 74.14\n",
            "VALIDATION:\n",
            "accuracy: 89.63, precision: 90.71, recall: 89.57, f1-score: 89.77\n",
            "LOSS train 1.0374763011932373 valid 0.8646896084149679\n",
            "EPOCH 49/500:\n",
            "TRAIN:\n",
            "accuracy: 74.20, precision: 74.39, recall: 74.27, f1-score: 74.25\n",
            "VALIDATION:\n",
            "accuracy: 89.63, precision: 90.71, recall: 89.57, f1-score: 89.77\n",
            "LOSS train 1.0533519321017795 valid 0.7662587364514669\n",
            "EPOCH 50/500:\n",
            "TRAIN:\n",
            "accuracy: 74.58, precision: 74.87, recall: 74.63, f1-score: 74.58\n",
            "VALIDATION:\n",
            "accuracy: 90.37, precision: 91.45, recall: 90.28, f1-score: 90.48\n",
            "LOSS train 1.033876982000139 valid 0.810591459274292\n",
            "EPOCH 51/500:\n",
            "TRAIN:\n",
            "accuracy: 73.45, precision: 73.83, recall: 73.53, f1-score: 73.56\n",
            "VALIDATION:\n",
            "accuracy: 90.37, precision: 91.36, recall: 90.28, f1-score: 90.48\n",
            "LOSS train 1.0537613497840033 valid 0.9089408119519552\n",
            "EPOCH 52/500:\n",
            "TRAIN:\n",
            "accuracy: 76.08, precision: 76.27, recall: 76.11, f1-score: 76.12\n",
            "VALIDATION:\n",
            "accuracy: 90.37, precision: 91.26, recall: 90.36, f1-score: 90.47\n",
            "LOSS train 1.018231623702579 valid 0.7872781753540039\n",
            "EPOCH 53/500:\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 44\u001b[0m\n\u001b[1;32m     40\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m LEARNING_RATE)\n\u001b[1;32m     42\u001b[0m lr_scheduler \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39mLR_SCHEDULER, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_metrics_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_metrics_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/Neural Network/nn-project/training/train.py:28\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(num_epochs, training_metrics_dict, validation_metrics_dict, training_loader, validation_loader, model, loss_fn, optimizer, plateau_scheduler, cfg)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Compute the average loss and the predictions vs true values\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Train the model for the single epoch\u001b[39;00m\n\u001b[1;32m     27\u001b[0m model\u001b[38;5;241m.\u001b[39mtraining_mode()\n\u001b[0;32m---> 28\u001b[0m loss_avg, y_pred_list, y_true_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Calculate the metrics\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTRAIN:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/Desktop/Neural Network/nn-project/training/train_one_epoch.py:11\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(training_loader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      8\u001b[0m y_true_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m loss_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m (training_loader):\n\u001b[1;32m     12\u001b[0m     \n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Divide the tuple data in inputs tensor and labels tensor (batch_size dimension)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m#print(data)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     x, y_true \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# extract waveform from tuple (waveform, sample_rate)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m#print(\"x_data\",x)\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/Desktop/Neural Network/nn-project/dataloader/dataset.py:72\u001b[0m, in \u001b[0;36mEMOVO_Dataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     70\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx]\n\u001b[1;32m     71\u001b[0m data \u001b[38;5;241m=\u001b[39m (data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), data[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 72\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, label\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Load the hyperparameters for training\n",
        "EPOCHS = cfg.train.num_epochs\n",
        "BATCH_SIZE = cfg.train.batch_size\n",
        "LEARNING_RATE = cfg.train.learning_rate\n",
        "OPTI = cfg.train.optimizer\n",
        "EARLY_STOPPING = cfg.train.early_stopping\n",
        "PATIENCE = cfg.train.patience\n",
        "LR_SCHEDULER = cfg.train.lr_scheduler\n",
        "LABEL_SMOOTHING = cfg.train.label_smoothing\n",
        "\n",
        "# Initialization of the metrics' dictionary\n",
        "training_metrics_dict = {\n",
        "    \"model\" : [model.model_name],\n",
        "    \"epoch\": [],\n",
        "    \"loss\": [],\n",
        "    \"accuracy\": [],\n",
        "    \"recall\": [],\n",
        "    \"precision\": [],\n",
        "    \"f1_score\": [],\n",
        "}\n",
        "validation_metrics_dict = {\n",
        "    \"model\" : [model.model_name],\n",
        "    \"epoch\": [],\n",
        "    \"loss\": [],\n",
        "    \"accuracy\": [],\n",
        "    \"recall\": [],\n",
        "    \"precision\": [],\n",
        "    \"f1_score\": [],\n",
        "}\n",
        "\n",
        "# Dataloader initialization for training, validation and test\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Definition of the Categorical Cross Entropy Loss\n",
        "loss_fn = nn.CrossEntropyLoss(label_smoothing = LABEL_SMOOTHING)\n",
        "\n",
        "# Definition of the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=LR_SCHEDULER, patience=5)\n",
        "\n",
        "train(\n",
        "    EPOCHS, \n",
        "    training_metrics_dict, \n",
        "    validation_metrics_dict, \n",
        "    train_dataloader, \n",
        "    val_dataloader, \n",
        "    model, \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    lr_scheduler, \n",
        "    cfg\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz43RZa3iuRu"
      },
      "source": [
        "# TESTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etj9SlVBiuRu"
      },
      "outputs": [],
      "source": [
        "# Path to the model.pt\n",
        "model_path = './testing/model.pt'\n",
        "\n",
        "# Batch size of the dataloader\n",
        "BATCH_SIZE = cfg.train.batch_size\n",
        "\n",
        "# Initialization of the dataloader\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Loss Function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialization of the test metrics dictionary\n",
        "test_metrics_dict = {\n",
        "    \"model\" : [model.model_name],\n",
        "    \"epoch\": [],\n",
        "    \"loss\": [],\n",
        "    \"accuracy\": [],\n",
        "    \"recall\": [],\n",
        "    \"precision\": [],\n",
        "    \"f1_score\": [],\n",
        "}\n",
        "\n",
        "cm = test(\n",
        "    model,\n",
        "    model_path,\n",
        "    test_dataloader,\n",
        "    test_metrics_dict,\n",
        "    loss_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivuBD21_iuRu"
      },
      "source": [
        "# Loss and Accuracy (Training and Validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjiZ1_DTiuRu"
      },
      "outputs": [],
      "source": [
        "actual_dir = './training/results/TIM2024-10-03_17:11:16'\n",
        "# TO DO add an intermidiate path with the directories of best models (1 for each possibility)\n",
        "path_training_metrics = actual_dir + '/training_metrics.json'\n",
        "path_validation_metrics = actual_dir+ '/validation_metrics.json'\n",
        "\n",
        "# load the dictionary {'model': [model_name], 'epoch': [1, 2 ...], 'loss': [1.9, 1.8 ...], 'accuracy': [0.6, 0.7, ...], 'recall': [0.2, 0.3, ...], 'precision': [0.3, 0.4, ...], 'f1-score': [0.5, 0.6, ...]}\n",
        "data_training = utils.load_metrics(path_training_metrics)\n",
        "data_validation = utils.load_metrics(path_validation_metrics)\n",
        "\n",
        "# Load the epochs, loss and accuracy in training and validation for the plots\n",
        "epochs = data_training['epoch']\n",
        "training_loss = data_training['loss']\n",
        "training_accuracy = data_training['accuracy']\n",
        "\n",
        "validation_loss = data_validation['loss']\n",
        "validation_accuracy = data_validation['accuracy']\n",
        "\n",
        "# Plot section\n",
        "# 1st Subplot => Loss in Training and Validation\n",
        "# 2nd Subplot => Accuracy in Training and Validation\n",
        "utils.plot_loss_acc(epochs, training_loss, validation_loss, training_accuracy, validation_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaPMYl_ViuRu"
      },
      "source": [
        "# Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oT4TIk1BiuRu"
      },
      "outputs": [],
      "source": [
        "# Plot the confusion Matrix\n",
        "\n",
        "class_names = ['dis', 'gio', 'neu', 'pau', 'rab', 'sor', 'tri']\n",
        "utils.plot_confusion_matrix(cm, class_names, cmap='rocket')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
