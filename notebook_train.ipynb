{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**NOTEBOOK NEURAL NETWORK EXAM** \n",
        "---\n",
        "\n",
        "<font size=\"5\">Title: <span style=\"color:red\">**C**</span>ontinuous <span style=\"color:red\">**k**</span>ernel <span style=\"color:red\">**T**</span>emporal-aware b<span style=\"color:red\">**I**</span>-direction <span style=\"color:red\">**M**</span>ulti-scale <span style=\"color:red\">**net**</span>work (CkTIM-net) for **Speech Emotion Recognition** (SER)</font>\n",
        "\n",
        "Authors: **Massimo Romano** (2043836), **Paolo Renzi** (1887793)\n",
        "\n",
        "<img src=\"./images/image_1.jpg\" alt=\"Description\" width=\"300\" height = \"300\" />\n",
        "\n",
        "<font size=\"5\">Introduction</font>\n",
        "\n",
        "<font size=\"3\"> **Speech Emotion Recognition** (SER) is the task that consists in automatically recognise the human emotion by voice signal. Technically speaking it's a **multi-class classification task**. This task plays a fundamental role in Machine Learning because of the growing importance of **HRI (Human Robot Interaction)**, in which we want algorithm that permits to the robot to recognise emotions and **improving communication with humans**.\n",
        "Traditional methods (before Deep Learning) tried to apply hand-crafted methods to extract significant features from speech signals, while Deep Learning methods tried to learn the class-discriminative features in an end-to-end manner, using various architectures such as Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) or the combination of CNN and RNN. \n",
        "</font>\n",
        "\n",
        "<font size=\"5\">TIM-net</font>\n",
        "\n",
        "**TIM-net** is a novel temporal emotional modeling approach to learn multi-scale contextual affective representations from various time scale with 3 main contributions: \n",
        "- a temporal-aware block based on the Dilated Causal Convolution (DC Conv) as a core unit in TIM-Net. \n",
        "The dilated convolution can enlarge and refine the receptive field of temporal patterns by padding the kernel and making it see samples further away at the cost of some closer by. The causal convolution combined with dilated convolution can help model relax the\n",
        "assumption of first-order Markov property compared with\n",
        "RNNs. In this way, they can incorporate an ùëÅ-order (ùëÅ\n",
        "denotes the number of all previous frames) connection into\n",
        "the network to aggregate information from different temporal\n",
        "locations.\n",
        "- a novel bi-direction architecture integrating complementary information from the past and the future for modeling long-range temporal dependencies. To the best of our knowledge, TIM-Net is the first bi-direction temporal network by focusing on multi-scale fusion in the SER, rather than simply concatenating forward and backward hidden states. \n",
        "- a dynamic fusion module by combining dynamic receptive fields for learning the inter-\n",
        "dependencies at different temporal scales, so as to improve\n",
        "the model generalizability. Due to the articulation speed and\n",
        "pause time varying significantly across speakers, the speech\n",
        "requires different efficient receptive fields (i.e., the time scale\n",
        "that reflects the affective characteristics) for each low-level\n",
        "feature (e.g., MFCC).\n",
        "\n",
        "\n",
        "<img src=\"./images/TIM-net.png\" alt=\"Description\" width=\"700\" height = \"300\" />\n",
        "\n",
        "\n",
        "<font size=\"5\">Proposed Method</font>\n",
        "\n",
        "A lot of deep learning architectures improve performances in this task: as an example TIM-net [1] that is a novel temporal emotional modeling approach to learn multi-scale contextual affective representations from various time scale. \n",
        "\n",
        "An interesting approach for convolutions is the Continuous Kernel Convolutions (CkConv) for sequential data [2], that permits to handles arbitrarily long sequences, solving the problem that standard Convolutional neural networks have, because they cannot handle sequences of unknown size and their memory horizon must be defined a priori.\n",
        "\n",
        "Our idea was to implement CkConv inside the TIM-net (both on Temporal Aware Blocks substituting the Dilated Convolutions and in the input substituting the Poinwise Convolutions) to decrease the depth of the network using less Temporal Aware Blocks (TABs), permitting to enlarge the receptive field also with a very less numbers of TABs in the network.\n",
        "\n",
        "The resulting network is:\n",
        "\n",
        "<img src=\"./images/cktim-net.jpg\" alt=\"Description\" width=\"300\" height = \"300\" />\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font size='5'>HOW TO USE THIS NOTEBOOK:</font>\n",
        "\n",
        "1) Change 'config.yaml' file:\n",
        "\n",
        "To initialize the model hyperparameters and the training hyperparameters you SHOULD change the config.yaml file in '/conf/config.yaml' directory\n",
        "\n",
        "In particular you can set:\n",
        "\n",
        "- train:\n",
        "\n",
        "  - num_epochs: 100, Ex. [1, 2, 3, ...]\n",
        "  \n",
        "  - batch_size: 64, Ex. [16, 32, 64, ..., len(dataset)]\n",
        "\n",
        "  - learning_rate: 0.01, Ex. [0.1, 0.01, 0.001, ...]\n",
        "\n",
        "  - patience: 5, Ex. [1, 2, 3, ...]\n",
        "\n",
        "  - lr_scheduler: 0.25, Ex. lr_scheduler belong to (0,1)\n",
        "  \n",
        "  - label_smoothing: 0.1, Ex. label_smoothing belong to (0,1)\n",
        "\n",
        "- model:\n",
        "\n",
        "  - dropout_rate: 0.1, Ex. [0.2 (20%), 0.3 (30%),...]\n",
        "\n",
        "  - n_temporal_aware_block: 6, Ex. [1, 2, 3, ...]\n",
        "\n",
        "  - ck: {True, False} (True if you want to use Continuous Kernel Convolutions in TIM-net, otherwise False)\n",
        "\n",
        "  - generator_type: {'conv', 'convKan'} (To use respectively Convolutional Kernel Generator or Convolutional KAN Kernel Generator in CkConv)\n",
        "\n",
        "  - kernel_size: 2, Ex. [2, 3, 4, 5, ...]\n",
        "\n",
        "  - omega_0: 25, Ex. [22.5, 24, 25, 27.35, ...]\n",
        "\n",
        "  - hidden_scale: 1, (needed to scale the trainable parameters of Convolution Kernel Generator of CkConv), Ex. [1 (100%), 0.1 (10%)...] \n",
        "\n",
        "  - af_type: {'sin', 'KAF', 'KAFsin', 'ReLu'} (type of the activation function in CkConv)\n",
        "\n",
        "  - aug: {True, False} (True if you want to use augmented dataset for training, False to use EMOVO.npy raw data)\n",
        "\n",
        " 2) Run:\n",
        "\n",
        "  You can run all the notebook to do the training both in 'local' and in 'colab', the code automatically recognise it.\n",
        "\n",
        "  If you are in 'local' the code automatically recognise what type of device you are using. The supported device are:\n",
        "  \n",
        "  - 'cuda': Nvidia GPUs\n",
        "\n",
        "  - 'mps': Apple Silicon GPUs (M1, M2 ...)\n",
        "\n",
        "  - 'cpu': No GPUs\n",
        "\n",
        "The result are:\n",
        "\n",
        "1) Download the dataset, Split it, Augment it\n",
        "\n",
        "2) Train the model in the configuration chosen by 'config.yaml' file and generate a directory in './training/results' with:\n",
        "  \n",
        "  -  'model.pt' file: weights of the model trained (to use in notebook_test.ipynb for testing)\n",
        "\n",
        "  -  'training_metrics.json': save the metrics (loss, accuracy, precision...) in training set\n",
        "\n",
        "  -  'validation_metrics.json': save the metrics (loss, accuracy, precision...) in validation set\n",
        "\n",
        "  -  'parameters.txt': save the configuration of the 'config.yaml' file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dIRawMyjYAz"
      },
      "source": [
        "‚ö†Ô∏èClone the repository to use all the functions needed by the main code‚ö†Ô∏è\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvEJSRQzjVVO",
        "outputId": "621797ba-2595-461d-e6ec-8c898391b5dd"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    mode = 'colab'\n",
        "except:\n",
        "    mode = 'local'\n",
        "\n",
        "if mode == 'colab':\n",
        "    !git clone \"https://github.com/cybernetic-m/nn-project.git\"\n",
        "    !pip install hydra-core --upgrade convkan\n",
        "else:\n",
        "    print(\"You are running locally!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZNKzy8qbbu2"
      },
      "source": [
        "# IMPORT AND INITIALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKakfnMRbKum"
      },
      "outputs": [],
      "source": [
        "# Import for config.yaml file\n",
        "from hydra import initialize, compose\n",
        "from hydra.core.global_hydra import GlobalHydra\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import Audio\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "if mode == 'colab':\n",
        "    sys.path.append('/content/nn-project/dataloader')\n",
        "    sys.path.append('/content/nn-project/model')\n",
        "    sys.path.append('/content/nn-project/module')\n",
        "    sys.path.append('/content/nn-project/training')\n",
        "    sys.path.append('/content/nn-project/testing')\n",
        "    from preprocessing import Preprocessing\n",
        "    import utils\n",
        "    import dataset\n",
        "    from ctim import CTIM\n",
        "    from train import *\n",
        "    from test import *\n",
        "\n",
        "\n",
        "if mode == 'local':\n",
        "    import dataloader.utils as utils\n",
        "    import dataloader.dataset as dataset\n",
        "    from dataloader.preprocessing import Preprocessing\n",
        "    from model.ctim import CTIM\n",
        "    from training.train import train as train\n",
        "    from testing.test import test as test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzpebiXBPJE-"
      },
      "source": [
        "Reproducibility\n",
        "\n",
        "This set a random seed to ensure that all the trials will be reproducible in the same way other times, obtaining same results in different runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3f4K2m8PIrc"
      },
      "outputs": [],
      "source": [
        "# Set the seed\n",
        "seed = 46\n",
        "\n",
        "# Set seed for torch, numpy and random libraries\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# Set the devide mode on GPU (if available CUDA for Nvidia and  MPS for Apple Silicon) or CPU\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mV42Zq-UKCz"
      },
      "source": [
        "## Download and Split Dataset\n",
        "\n",
        "EMOVO is a dataset composed of 588 samples of recordings of stereo audio sampled at 48khz and saved in wav format.\n",
        "Each sample is a phrase pronounced by 6 actors (3 males and 3 females) with ids as m1-3 and f1-3.\n",
        "\n",
        "Each phrase has an id based on:\n",
        "- b1, b2, b3 brevi short\n",
        "- l1, l2, l3, l4 lunghe long\n",
        "- n1, n2, n3, n4, n5 nonsense\n",
        "- d1, d2 domande (questions)\n",
        "\n",
        "and each emotion is identified like this:\n",
        "- *neu*tro neutral\n",
        "- *dis*gusto disgust\n",
        "- *gio*ia joy\n",
        "- *pau*ra fear\n",
        "- *rab*bia anger\n",
        "- *sor*presa surprise\n",
        "- *tri*stezza sadness\n",
        "\n",
        "Each file then is identified like this emotionid-actorid-phraseid.wav\n",
        "\n",
        "More information about the dataset is available on \"./papers/emovo.pdf\"\n",
        "\n",
        "To split the dataset we first put samples with the same class in the same directory, Then we counted how many samples we had in each class and multiplied it by the percentage of samples we wanted in that split. We then created a list of indexes of that length to get the file with those indexes from the folders of each class to have a balanced random selection of files.\n",
        "\n",
        "For the split we have chosen:\n",
        "\n",
        "- 70 % of samples for Training set\n",
        "- 20 % of samples for Test set\n",
        "- 10 % of sample for Validation set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV_poPYiccs2",
        "outputId": "e50c5a1d-7044-4167-99c3-48ec2f05c711"
      },
      "outputs": [],
      "source": [
        "link_dataset = \"https://drive.google.com/file/d/1nzKBta2M3khw7Ql_S7atYg-H-bWDiOxr/view?usp=drive_link\"\n",
        "gdrive_link = \"https://drive.google.com/uc?export=download&id=\"\n",
        "\n",
        "if mode == 'colab':\n",
        "    destination_dir = \"/content/emovo.zip\"\n",
        "    extract_dir = '/content/dataset'\n",
        "    emovo_dir = '/content/dataset/EMOVO'\n",
        "    emovo_split_dir = '/content/dataset/EMOVO_split'\n",
        "elif mode == 'local':\n",
        "    destination_dir = \"./emovo.zip\"\n",
        "    extract_dir = \"./dataset\"\n",
        "    emovo_dir = \"./dataset/EMOVO\"\n",
        "    emovo_split_dir = './dataset/EMOVO_split'\n",
        "\n",
        "utils.download_dataset(link_dataset, destination_dir, gdrive_link, extract_dir)\n",
        "\n",
        "utils.dataset_split(emovo_dir, extract_dir, 0.7, 0.2, 0.1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Split the MFCC dataset\n",
        "\n",
        "We manged to find also the dataset already processed by the authors of TIM-net and we split it following a similar idea to how we splitted the original dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if mode == 'colab':\n",
        "    utils.dataset_split_mfcc('./nn-project/EMOVO.npy', extract_dir, 0.7, 0.2, 0.1)\n",
        "elif mode == 'local':\n",
        "    utils.dataset_split_mfcc('./EMOVO.npy', extract_dir, 0.7, 0.2, 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrZ8n_tp2nfh"
      },
      "source": [
        "# Data Augmentation\n",
        "\n",
        "To augment the data we selected 4 transformations and applied all of them to each sample to augment the dataset as much as possible. The selected transformations where:\n",
        "\n",
        "- White Noise adding\n",
        "- Shifting the signal either to left or right randomly, by padding one side and cutting the other\n",
        "- Speeding up a track and changing the pitch of a random amount \n",
        "- Reversing the track"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBRzIeeY2nfh",
        "outputId": "b7a2bfe4-c0e0-40ab-94c7-f9a3afffa6d9"
      },
      "outputs": [],
      "source": [
        "preprocessing = Preprocessing(device=device)\n",
        "\n",
        "utils.augment_data(emovo_split_dir, extract_dir, preprocessing, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVRKvN26UUq9"
      },
      "source": [
        "## Dataset Exploration\n",
        "\n",
        "In this part we'll see the original dataset, the augmented dataset and different examples of audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRn6VibgUXm9",
        "outputId": "491fc476-55ab-423a-f653-619cae3944dc"
      },
      "outputs": [],
      "source": [
        "train_path = os.path.join(extract_dir, 'EMOVO_split', 'train')\n",
        "test_path = os.path.join(extract_dir, 'EMOVO_split', 'test')\n",
        "val_path = os.path.join(extract_dir, 'EMOVO_split', 'val')\n",
        "\n",
        "train_dataset = dataset.EMOVO_Dataset(train_path, feature_extract=False, mfcc_np=False, device=device)\n",
        "test_dataset = dataset.EMOVO_Dataset(test_path, feature_extract=False, mfcc_np=False, device=device)\n",
        "val_dataset = dataset.EMOVO_Dataset(val_path, feature_extract=False, mfcc_np=False, device=device)\n",
        "\n",
        "train_aug_path = os.path.join(extract_dir, 'EMOVO_aug', 'train')\n",
        "test_aug_path = os.path.join(extract_dir, 'EMOVO_aug', 'test')\n",
        "val_aug_path = os.path.join(extract_dir, 'EMOVO_aug', 'val')\n",
        "\n",
        "train_aug_dataset = dataset.EMOVO_Dataset(train_aug_path, feature_extract=False, mfcc_np=False, device=device)\n",
        "test_aug_dataset = dataset.EMOVO_Dataset(test_aug_path,feature_extract=False, mfcc_np=False, device=device)\n",
        "val_aug_dataset = dataset.EMOVO_Dataset(val_aug_path, feature_extract=False, mfcc_np=False, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ9URMKkfQmU"
      },
      "source": [
        "<font size='5'>**Example of audio in original dataset**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "mcblPPkeZw35",
        "outputId": "f79994e1-f311-4cb7-f112-de385323b3ae"
      },
      "outputs": [],
      "source": [
        "classes = ['dis', 'gio', 'neu', 'pau', 'rab', 'sor', 'tri']\n",
        "random_num = random.randint(1, train_dataset.__len__())\n",
        "data, label = train_dataset[random_num]\n",
        "\n",
        "print(\"Tensor of shape:\",data[0].shape, \"Sample Rate:\", data[1])\n",
        "print(\"Class:\", classes[label], \"\\n\")\n",
        "\n",
        "# Waveform\n",
        "# Compute the average of both channels to get a mono waveform\n",
        "mono_waveform = data[0].mean(dim=0)\n",
        "num_samples = mono_waveform.shape[0]\n",
        "sample_rate = data[1]\n",
        "time_axis = np.linspace(0, num_samples / sample_rate, num_samples)\n",
        "\n",
        "# Play the audio\n",
        "display(Audio(mono_waveform.cpu().numpy(), rate=sample_rate))\n",
        "\n",
        "# Plot the averaged (mono) waveform\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(time_axis, mono_waveform.cpu().numpy())\n",
        "plt.title('Waveform (Averaged Mono)')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSzFkhBu2nfi"
      },
      "source": [
        "<font size='5'>**Example of audio in augmented dataset**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5XiV4x82nfi"
      },
      "outputs": [],
      "source": [
        "classes = ['dis', 'gio', 'neu', 'pau', 'rab', 'sor', 'tri']\n",
        "random_num = random.randint(1, train_dataset.__len__())\n",
        "data, label = train_aug_dataset[random_num]\n",
        "\n",
        "print(\"Tensor of shape:\",data[0].shape, \"Sample Rate:\", data[1])\n",
        "print(\"Class:\", classes[label], \"\\n\")\n",
        "\n",
        "# Waveform\n",
        "# Compute the average of both channels to get a mono waveform\n",
        "mono_waveform = data[0].mean(dim=0)\n",
        "num_samples = mono_waveform.shape[0]\n",
        "sample_rate = data[1]\n",
        "time_axis = np.linspace(0, num_samples / sample_rate, num_samples)\n",
        "\n",
        "# Play the audio\n",
        "display(Audio(mono_waveform.cpu().numpy(), rate=sample_rate))\n",
        "\n",
        "# Plot the averaged (mono) waveform\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(time_axis, mono_waveform.cpu().numpy())\n",
        "plt.title('Waveform (Averaged Mono)')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnwkpxVjIDni"
      },
      "source": [
        "<font size='5'>**Data distribution (original)**</font>\n",
        "\n",
        "In this part we can see the original dataset distribution after the split in training, validation and test set.\n",
        "\n",
        "- 58 samples x 7 classes = 406 samples (Training)\n",
        "\n",
        "- 10 samples x 7 classes = 70 samples (Validation)\n",
        "\n",
        "- 16 samples x 7 classes =  112 samples (Test)\n",
        "\n",
        "                         558 samples (TOT)\n",
        "\n",
        "                            \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "q3Hs-83fIHPT",
        "outputId": "f64ac95c-b03e-4b4b-9156-c7e93f23e3b8"
      },
      "outputs": [],
      "source": [
        "train_counts = train_dataset.get_info()\n",
        "test_counts = test_dataset.get_info()\n",
        "val_counts = val_dataset.get_info()\n",
        "classes = ['dis', 'gio', 'neu', 'pau', 'rab', 'sor', 'tri']\n",
        "\n",
        "fig, ax = plt.subplots(1,3, figsize=(16, 4))\n",
        "\n",
        "ax[0].bar(classes, train_counts, color='limegreen')\n",
        "ax[0].set_title(\"Data distribution in Training Set\")\n",
        "ax[0].set_xlabel(\"Classes\")\n",
        "ax[0].set_xticks(ticks=range(7), labels=classes)\n",
        "ax[0].set_ylabel(\"Number of samples\")\n",
        "\n",
        "ax[1].bar(classes, test_counts, color='seagreen')\n",
        "ax[1].set_title(\"Data distribution in Test Set\")\n",
        "ax[1].set_xlabel(\"Classes\")\n",
        "ax[1].set_xticks(ticks=range(7), labels=classes)\n",
        "ax[1].set_ylabel(\"Number of samples\")\n",
        "\n",
        "ax[2].bar(classes, val_counts, color='olive')\n",
        "ax[2].set_title(\"Data distribution in Validation Set\")\n",
        "ax[2].set_xlabel(\"Classes\")\n",
        "ax[2].set_xticks(ticks=range(7), labels=classes)\n",
        "ax[2].set_ylabel(\"Number of samples\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzyjYYHp2nfi"
      },
      "source": [
        "<font size='5'>**Data distribution (after augmentation)**</font>\n",
        "\n",
        "We have applied 4 different transformations (described in augmentation section of this notebook) to each sample of Training Set. \n",
        "\n",
        "The result distribution is:\n",
        "\n",
        "- [58 samples + (58 samples x 4 transformations)] x 7 classes = 2030 samples (Training)\n",
        "\n",
        "- 10 samples x 7 classes = 70 samples (Validation)\n",
        "\n",
        "- 16 samples x 7 classes =  112 samples (Test)\n",
        "\n",
        "                         2212 samples (TOT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "WUqtdsOf2nfi",
        "outputId": "3d60a842-b1f0-41c5-f68d-d605dbfcbd14"
      },
      "outputs": [],
      "source": [
        "train_aug_counts = train_aug_dataset.get_info()\n",
        "test_aug_counts = test_aug_dataset.get_info()\n",
        "val_aug_counts = val_aug_dataset.get_info()\n",
        "classes = ['dis', 'gio', 'neu', 'pau', 'rab', 'sor', 'tri']\n",
        "\n",
        "fig, ax = plt.subplots(1,3, figsize=(16, 4))\n",
        "\n",
        "ax[0].bar(classes, train_aug_counts, color='limegreen')\n",
        "ax[0].set_title(\"Data distribution in Training Set\")\n",
        "ax[0].set_xlabel(\"Classes\")\n",
        "ax[0].set_xticks(ticks=range(7), labels=classes)\n",
        "ax[0].set_ylabel(\"Number of samples\")\n",
        "\n",
        "ax[1].bar(classes, test_aug_counts, color='seagreen')\n",
        "ax[1].set_title(\"Data distribution in Test Set\")\n",
        "ax[1].set_xlabel(\"Classes\")\n",
        "ax[1].set_xticks(ticks=range(7), labels=classes)\n",
        "ax[1].set_ylabel(\"Number of samples\")\n",
        "\n",
        "ax[2].bar(classes, val_aug_counts, color='olive')\n",
        "ax[2].set_title(\"Data distribution in Validation Set\")\n",
        "ax[2].set_xlabel(\"Classes\")\n",
        "ax[2].set_xticks(ticks=range(7), labels=classes)\n",
        "ax[2].set_ylabel(\"Number of samples\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6u2yOd0iuRt"
      },
      "source": [
        "# MODEL INITIALIZATION\n",
        "\n",
        "To initialize the model we have used Hydra with a \"config.yaml\" ('./conf/config.yaml') file in which we have defined all the hyperparameters (batch_size, learning_rate ...) both for the model and training.\n",
        "\n",
        "You SHOULD change the parameters before training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LK9-Zzs-iuRt",
        "outputId": "e32700b7-b64a-4601-cd21-6b343b4a1365"
      },
      "outputs": [],
      "source": [
        "# Check if Hydra is initialized\n",
        "if GlobalHydra().is_initialized():\n",
        "    # Clear the Hydra instance if it is initialized\n",
        "    GlobalHydra.instance().clear()\n",
        "    print(\"Hydra instance was initialized and has been cleared.\")\n",
        "else:\n",
        "    # Initialize\n",
        "    print(\"Hydra now initialized!\")\n",
        "\n",
        "# Initialization and Load configuration\n",
        "if mode == 'local':\n",
        "  initialize(config_path=\"./conf\", job_name=\"notebook_nn_exam\", version_base=None)\n",
        "  cfg = compose(config_name=\"config\")\n",
        "\n",
        "elif mode == 'colab':\n",
        "  initialize(config_path=\"./nn-project/conf\", job_name=\"notebook_nn_exam\", version_base=None)\n",
        "  cfg = compose(config_name=\"config\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vn5O9hp_iuRt"
      },
      "outputs": [],
      "source": [
        "# Load the hyperparameters of the model\n",
        "DROPOUT_RATE = cfg.model.dropout_rate\n",
        "KERNEL_SIZE = cfg.model.kernel_size\n",
        "N_TAB = cfg.model.n_temporal_aware_block\n",
        "CK = cfg.model.ck\n",
        "GEN_TYPE = cfg.model.generator_type\n",
        "OMEGA_0 = cfg.model.omega_0\n",
        "AF_TYPE = cfg.model.af_type\n",
        "AUG = cfg.model.aug\n",
        "HIDDEN_SCALE = cfg.model.hidden_scale\n",
        "\n",
        "\n",
        "model = CTIM(\n",
        "    kernel_size=KERNEL_SIZE,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    n_temporal_aware_block=N_TAB,\n",
        "    n_filter=39,\n",
        "    in_channels=39,\n",
        "    ck=CK,\n",
        "    num_features=39,\n",
        "    num_classes = 7,\n",
        "    generator_type= GEN_TYPE,\n",
        "    omega_0=OMEGA_0,\n",
        "    af_type=AF_TYPE,\n",
        "    hidden_scale=HIDDEN_SCALE,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUdCPiRZiuRu"
      },
      "source": [
        "# TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EmPUS-iBiuRu",
        "outputId": "58030685-a642-4cea-ff80-09af8c680bac"
      },
      "outputs": [],
      "source": [
        "# Load the hyperparameters for training\n",
        "EPOCHS = cfg.train.num_epochs\n",
        "BATCH_SIZE = cfg.train.batch_size\n",
        "LEARNING_RATE = cfg.train.learning_rate\n",
        "PATIENCE = cfg.train.patience\n",
        "LR_SCHEDULER = cfg.train.lr_scheduler\n",
        "LABEL_SMOOTHING = cfg.train.label_smoothing\n",
        "\n",
        "# Initialization of the metrics' dictionary\n",
        "training_metrics_dict = {\n",
        "    \"model\" : [model.model_name],\n",
        "    \"epoch\": [],\n",
        "    \"loss\": [],\n",
        "    \"accuracy\": [],\n",
        "    \"recall\": [],\n",
        "    \"precision\": [],\n",
        "    \"f1_score\": [],\n",
        "}\n",
        "validation_metrics_dict = {\n",
        "    \"model\" : [model.model_name],\n",
        "    \"epoch\": [],\n",
        "    \"loss\": [],\n",
        "    \"accuracy\": [],\n",
        "    \"recall\": [],\n",
        "    \"precision\": [],\n",
        "    \"f1_score\": [],\n",
        "}\n",
        "\n",
        "if AUG == True:\n",
        "    # Dataloader initialization for training, validation and test\n",
        "    train_path = os.path.join(extract_dir, 'EMOVO_aug', 'train')\n",
        "    val_path = os.path.join(extract_dir, 'EMOVO_aug', 'val')\n",
        "\n",
        "    train_dataset = dataset.EMOVO_Dataset(train_path, feature_extract=True, mfcc_np=False, device=device)\n",
        "    val_dataset = dataset.EMOVO_Dataset(val_path, feature_extract=True, mfcc_np=False, device=device)\n",
        "\n",
        "elif AUG == False:\n",
        "\n",
        "    train_path = os.path.join(extract_dir, 'EMOVO_split_MFCC', 'train')\n",
        "    val_path = os.path.join(extract_dir, 'EMOVO_split_MFCC', 'val')\n",
        "\n",
        "    train_dataset = dataset.EMOVO_Dataset(train_path, feature_extract=False, mfcc_np=True, device=device)\n",
        "    val_dataset = dataset.EMOVO_Dataset(val_path, feature_extract=False, mfcc_np=True, device=device)\n",
        "\n",
        "\n",
        "# Dataloader initialization\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Definition of the Categorical Cross Entropy Loss\n",
        "loss_fn = nn.CrossEntropyLoss(label_smoothing = LABEL_SMOOTHING)\n",
        "\n",
        "# Definition of the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=LR_SCHEDULER, patience=5)\n",
        "\n",
        "train(\n",
        "    EPOCHS, \n",
        "    training_metrics_dict, \n",
        "    validation_metrics_dict, \n",
        "    train_dataloader, \n",
        "    val_dataloader, \n",
        "    model, \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    lr_scheduler, \n",
        "    cfg\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "References\n",
        "---\n",
        "[1] [TEMPORAL MODELING MATTERS: A NOVEL TEMPORAL EMOTIONAL MODELING\n",
        "APPROACH FOR SPEECH EMOTION RECOGNITION](https://arxiv.org/pdf/2211.08233)\n",
        "\n",
        "[2] [CKCONV: CONTINUOUS KERNEL CONVOLUTION FOR\n",
        "SEQUENTIAL DATA](https://arxiv.org/pdf/2102.02611)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
