{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font size='7'>NOTEBOOK TEST</font>** \n",
    "\n",
    "Authors: **Massimo Romano** (2043836), **Paolo Renzi** (1887793)\n",
    "\n",
    "This notebook tests a trained model on never before seen data, computes all the relevant metrics:\n",
    "\n",
    "- *accuracy*: is the fraction of correctly classified instances among all the instances \n",
    "\n",
    "<img src=\"./images/Accuracy.png\" alt=\"Description\" width=\"200\" height = \"60\" />\n",
    "\n",
    "- *precision (macro)*: is the fraction of True Positive elements divided by the total number of positively **predicted by the model** samples, averaged over each class \n",
    "\n",
    "<img src=\"./images/MacroPrecision.png\" alt=\"Description\" width=\"250\" height = \"60\" />\n",
    "\n",
    "- *recall (macro)*: is the fraction of True Positive elements divided by the total number of positively **labeled** samples, averaged over each class \n",
    "\n",
    "<img src=\"./images/MacroRecall.png\" alt=\"Description\" width=\"250\" height = \"60\" />\n",
    "\n",
    "- *f1-score (macro)*: is the harmonic mean of Precision(macro) and Recall(macro) \n",
    "\n",
    "<img src=\"./images/F1score.webp\" alt=\"Description\" width=\"250\" height = \"60\" />\n",
    "\n",
    "- *loss*: cross-entropy loss, measure of distances between probability distributions y and y_hat\n",
    "\n",
    "<img src=\"./images/Cross-entropy.png\" alt=\"Description\" width=\"200\" height = \"60\" />\n",
    "\n",
    "- *inference_time_one_sample*: the time it take the model to compute the logits on a single sample\n",
    "\n",
    "- *inference_time_tot*: the sum of the time it take the model to compute the logits on all the samples of the dataset\n",
    "\n",
    "we choose the macro versions instead of the micro for precision, recall and f1-score because our classes are balanced \n",
    "\n",
    "saves all of them in a json file and 2 images, one for the accuracy and loss and the other for the confusion matrix.\n",
    "\n",
    "To do so you need a dir with the weights of the model saved in a pt file, and 2 json files with the metrics saved during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "Change the path to the model's directory that you want to test and put the right hyper-parameters to create the right model (some are taken from the directory's name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THIS PART TO TEST DIFFERENT MODELS!!!\n",
    "# The only parameters to change are the directory path of the model to test\n",
    "# and the Activation Function type\n",
    "# ****---------------------------------------****\n",
    "actual_dir = \"./results/TIM_TAB6\"\n",
    "AF_TYPE = 'sin'\n",
    "HIDDEN_SCALE = 1\n",
    "N_TAB = 6\n",
    "# ****---------------------------------------****\n",
    "# These other are automatic based on the name of the directory!!!\n",
    "# Ex. CkkTIM_aug, CkTIM (no augmentation) ...\n",
    "GEN_TYPE = 'convKan' if 'Ckk' in actual_dir else 'conv'\n",
    "CK = 'Ck' in actual_dir \n",
    "AUG = 'aug' in actual_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import initialize, compose\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import dataloader.utils as utils\n",
    "import dataloader.dataset as dataset\n",
    "from dataloader.preprocessing import Preprocessing\n",
    "from model.ctim import CTIM\n",
    "from testing.test import test as test\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Check if Hydra is initialized\n",
    "if GlobalHydra().is_initialized():\n",
    "    # Clear the Hydra instance if it is initialized\n",
    "    GlobalHydra.instance().clear()\n",
    "    print(\"Hydra instance was initialized and has been cleared.\")\n",
    "else:\n",
    "    # Initialize\n",
    "    print(\"Hydra now initialized!\")\n",
    "\n",
    "# Initialization and Load configuration\n",
    "initialize(config_path=\"./conf\", job_name=\"notebook_nn_exam\", version_base=None)\n",
    "cfg = compose(config_name=\"config\")\n",
    "\n",
    "# Set the devide mode on GPU (if available CUDA for Nvidia and  MPS for Apple Silicon) or CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Your device is:\", device)\n",
    "\n",
    "# Load the hyperparameters of the model\n",
    "DROPOUT_RATE = cfg.model.dropout_rate\n",
    "KERNEL_SIZE = cfg.model.kernel_size\n",
    "N_FILTER = cfg.model.n_filter\n",
    "NUM_FEATURES = cfg.model.num_features\n",
    "OMEGA_0 = cfg.model.omega_0\n",
    "\n",
    "\n",
    "model = CTIM(\n",
    "    kernel_size=KERNEL_SIZE,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    n_temporal_aware_block=N_TAB,\n",
    "    n_filter=N_FILTER,\n",
    "    in_channels=39,\n",
    "    ck=CK,\n",
    "    generator_type= GEN_TYPE,\n",
    "    num_features=NUM_FEATURES,\n",
    "    num_classes = 7,\n",
    "    omega_0=OMEGA_0,\n",
    "    af_type=AF_TYPE,\n",
    "    hidden_scale=HIDDEN_SCALE,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "model_path = actual_dir + '/model.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Summary\n",
    "\n",
    "Prints a summary of the model's layers with the number of parameters for each part of the layer and a total count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = utils.count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST \n",
    "\n",
    "Computes all the metrics on the test set precision, recall, accuracy, f1-score and saves them in a json file,  including the inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if AUG == True:\n",
    "    test_path = './dataset/EMOVO_aug/test'\n",
    "    test_dataset = dataset.EMOVO_Dataset(test_path, feature_extract=True, mfcc_np=False, device=device)\n",
    "\n",
    "    # Initialization of the dataloader\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    # Loss Function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialization of the test metrics dictionary\n",
    "    test_metrics_dict = {\n",
    "        \"model\" : [model.model_name],\n",
    "        \"epoch\": [],\n",
    "        \"loss\": [],\n",
    "        \"accuracy\": [],\n",
    "        \"recall\": [],\n",
    "        \"precision\": [],\n",
    "        \"f1_score\": [],\n",
    "        \"inference_time_one_sample\": [],\n",
    "        \"inference_time_tot\": [],\n",
    "        \"trainable_parameters\": [parameters]\n",
    "    }\n",
    "\n",
    "    cm, inference_time_list = test(\n",
    "        model,\n",
    "        model_path,\n",
    "        test_dataloader,\n",
    "        test_metrics_dict,\n",
    "        loss_fn\n",
    "    )\n",
    "\n",
    "elif AUG == False:\n",
    "\n",
    "    test_path = './dataset/EMOVO_split_MFCC/test'\n",
    "    test_dataset = dataset.EMOVO_Dataset(test_path, feature_extract=False, mfcc_np=True, device=device)\n",
    "\n",
    "    # Initialization of the dataloader\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    # Loss Function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialization of the test metrics dictionary\n",
    "    test_metrics_dict = {\n",
    "        \"model\" : [model.model_name],\n",
    "        \"epoch\": [],\n",
    "        \"loss\": [],\n",
    "        \"accuracy\": [],\n",
    "        \"recall\": [],\n",
    "        \"precision\": [],\n",
    "        \"f1_score\": [],\n",
    "        \"inference_time_one_sample\": [],\n",
    "        \"inference_time_tot\": [],\n",
    "        \"trainable_parameters\": [parameters]\n",
    "    }\n",
    "\n",
    "    cm, inference_time_list = test(\n",
    "        model,\n",
    "        model_path,\n",
    "        test_dataloader,\n",
    "        test_metrics_dict,\n",
    "        loss_fn\n",
    "    )\n",
    "    \n",
    "inference_time_one_sample = round(torch.tensor(inference_time_list).mean().item(),3)\n",
    "inference_time_tot = round(torch.tensor(inference_time_list).sum().item(),3)\n",
    "\n",
    "print(\"Inference Time for 1 sample (average value):\")\n",
    "print(inference_time_one_sample)\n",
    "print(\"Inference Time for all samples of test set:\")\n",
    "print(inference_time_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and Accuracy (Training and Validation)\n",
    "\n",
    "Plots and saves a graph of the accuracy and loss over the epochs in the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_training_metrics = actual_dir + '/training_metrics.json'\n",
    "path_validation_metrics = actual_dir+ '/validation_metrics.json'\n",
    "\n",
    "# load the dictionary {'model': [model_name], 'epoch': [1, 2 ...], 'loss': [1.9, 1.8 ...], 'accuracy': [0.6, 0.7, ...], 'recall': [0.2, 0.3, ...], 'precision': [0.3, 0.4, ...], 'f1-score': [0.5, 0.6, ...]}\n",
    "data_training = utils.load_metrics(path_training_metrics)\n",
    "data_validation = utils.load_metrics(path_validation_metrics)\n",
    "\n",
    "# Load the epochs, loss and accuracy in training and validation for the plots\n",
    "epochs = data_training['epoch']\n",
    "training_loss = data_training['loss']\n",
    "training_accuracy = data_training['accuracy']\n",
    "\n",
    "validation_loss = data_validation['loss']\n",
    "validation_accuracy = data_validation['accuracy']\n",
    "\n",
    "# Plot section\n",
    "# 1st Subplot => Loss in Training and Validation\n",
    "# 2nd Subplot => Accuracy in Training and Validation\n",
    "utils.plot_loss_acc(epochs, training_loss, validation_loss, training_accuracy, validation_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "Plots and saves the confusion matrix for the test set, showing how the samples are classified compared to their true label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion Matrix\n",
    "class_names = ['dis', 'gio', 'neu', 'pau', 'rab', 'sor', 'tri']\n",
    "utils.plot_confusion_matrix(cm, class_names, cmap='rocket')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
